<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: October 23, 2025 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.c30d8a14377b0418a6b56a64af4e417d.css" />
  <link rel="stylesheet" href="/css/shared-layout.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  























  
  
  






  <meta name="author" content="Manling Li" />





  

<meta name="description" content="Manling Li" />



<link rel="alternate" hreflang="en-us" href="https://limanling.github.io/students/" />
<link rel="canonical" href="https://limanling.github.io/students/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#714ABB" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@ManlingLi_" />
  <meta property="twitter:creator" content="@ManlingLi_" />
<meta property="twitter:image" content="https://limanling.github.io/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Manling Li" />
<meta property="og:url" content="https://limanling.github.io/students/" />
<meta property="og:title" content="Manling Li" />
<meta property="og:description" content="Manling Li" /><meta property="og:image" content="https://limanling.github.io/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  










  
  
  

  
  
    <link rel="alternate" href="/students/index.xml" type="application/rss+xml" title="Manling Li" />
  

  


  
  <title>Manling Li</title>

  
  
  
  











  <style id="codex-footer-style">
    .site-footer {
      margin-top: 3rem;
      border-top: 1px solid #e6e6e6;
      background-color: #fafafa;
      padding: 2rem 1rem 2.5rem;
      text-align: center;
      font-size: 0.95rem;
      color: #4b4b4b;
    }
    .site-footer .footer-inner {
      max-width: 820px;
      margin: 0 auto;
    }
    .site-footer .footer-title {
      font-weight: 600;
      color: #2c2c2c;
    }
    .site-footer p {
      margin: 0.35rem 0;
    }
    .site-footer .footer-meta {
      font-size: 0.85rem;
      color: #6d6d6d;
    }
    .site-footer a {
      color: #5b4dd6;
      font-weight: 600;
      text-decoration: none;
    }
    .site-footer a:hover,
    .site-footer a:focus {
      text-decoration: underline;
    }
    
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
                      line-height: 1.6;
                      color: #333;
                      max-width: 1200px;
                      margin: 0 auto;
                      padding: 20px;
                      background-color: #fff;
                  }
                  
                  .universal-wrapper {
                      padding: 0 20px;
                  }
                  
                  .article-style {
                      font-size: 16px;
                      line-height: 1.8;
                  }
                  
                  h1 {
                      font-size: 2.5rem;
                      margin-bottom: 0.5rem;
                      color: #2c3e50;
                      border-bottom: 3px solid #3498db;
                      padding-bottom: 10px;
                  }
                  
                  h2 {
                      font-size: 1.8rem;
                      margin-top: 2rem;
                      margin-bottom: 1rem;
                      color: #34495e;
                      border-bottom: 2px solid #e0e0e0;
                      padding-bottom: 8px;
                  }
                  
                  h3 {
                      font-size: 1.4rem;
                      margin-top: 1.5rem;
                      margin-bottom: 0.8rem;
                      color: #555;
                  }
                  
                  .author-name {
                      font-size: 1.2rem;
                      color: #666;
                      margin-bottom: 2rem;
                  }
                  
                  .research-vision {
                      background-color: #f8f9fa;
                      padding: 20px;
                      border-left: 4px solid #3498db;
                      margin: 20px 0;
                  }
                  
                  .section-container {
                      margin: 30px 0;
                  }
                  
                  ul {
                      margin: 15px 0;
                      padding-left: 30px;
                  }
                  
                  li {
                      margin: 8px 0;
                  }
                  
                  .key-contributions {
                      background-color: #fff;
                      border: 1px solid #dee2e6;
                      border-radius: 8px;
                      padding: 20px;
                      margin: 20px 0;
                  }
                  
                  .highlight {
                      background-color: #ffffcc;
                      padding: 2px 4px;
                  }
                  
                  .figure-caption {
                      font-size: 0.9rem;
                      color: #666;
                      text-align: center;
                      margin-top: 10px;
                      font-style: italic;
                  }
                  
                  .awards-section {
                      background-color: #f0f8ff;
                      padding: 15px;
                      border-radius: 8px;
                      margin: 20px 0;
                  }
                  
                  .citation {
                      font-size: 0.9rem;
                      color: #555;
                      padding-left: 20px;
                      border-left: 3px solid #ddd;
                      margin: 10px 0;
                  }
                  
                  strong {
                      color: #2c3e50;
                      font-weight: 600;
                  }
                  
                  .research-area {
                      display: inline-block;
                      background-color: #e3f2fd;
                      padding: 4px 12px;
                      border-radius: 4px;
                      margin: 4px;
                      font-size: 0.9rem;
                  }
                  
                  .future-research {
                      background-color: #f5f5f5;
                      padding: 20px;
                      border-radius: 8px;
                      margin-top: 30px;
                  }
                  
                  .references {
                      font-size: 0.85rem;
                      line-height: 1.4;
                      margin-top: 40px;
                      padding-top: 20px;
                      border-top: 2px solid #e0e0e0;
                  }
                  
                  .reference-item {
                      margin: 8px 0;
                      padding-left: 20px;
                      text-indent: -20px;
                  }
              
  </style>

</head>


 <body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   " data-wc-page-id="2c778b35ac809d331bccf991419ec2ed" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <!-- SHARED NAV START -->
  <div class="page-header header--fixed">
<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Manling Li</a>
      </div>
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Manling Li</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
              
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="https://www.mll.lab.northwestern.edu/" target="_blank" rel="noopener"><span>MLL-Lab</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
          </li>

          <li class="nav-item">
            <a class="nav-link " href="/#publications" data-target="#publications"><span>Publications</span></a>
          </li>
          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          



          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#services" data-target="#services"><span>Services</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#teaching" data-target="#teaching"><span>Teaching</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#students" data-target="#students"><span>Students</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>
  <!-- SHARED NAV END -->

  <div class="page-body">
    <span class="js-widget-page d-none"></span>
    <div class="site-layout container-xl">
      <div class="row">
        <!-- SHARED SIDEBAR START -->
      <div class="col-12 col-lg-3 site-profile sticky-column">
          <div id="profile" class="profile-card">
           
                 
                 
                 <img class="avatar avatar-circle" src="/authors/admin/avatar_hudd2cb509b9271810998b42a25e34ae22_4743348_270x270_fill_q75_lanczos_center.jpg" alt="Avatar">
                 

                 <h2>Manling Li</h2>
                 <p class="profile-role">Assistant Professor<br>Northwestern University</p>

                 <div class="profile-summary">
                   <p class="profile-primary">
                     <a href="https://www.mccormick.northwestern.edu/computer-science/" target="_blank" rel="noopener">Computer Science</a>
                   </p>
                   <!-- <p class="profile-affiliation">
                     <a href="https://cogsci.northwestern.edu" target="_blank" rel="noopener">Cognitive Science Program</a>
                     <span>(Affiliated)</span>
                   </p> -->
                   <p class="profile-affiliation">
                     <a href="https://robotics.northwestern.edu" target="_blank" rel="noopener">Center for Robotics and Biosystems</a>
                     <span>(Affiliated)</span>
                   </p>
                 </div>
                 <!-- <div class="profile-tagline"> -->
 
                 <!-- </div> -->
                <div class="profile-tagline">
                  <p class="profile-role">
                    Amazon Scholar
                  </p>
                  <a href="https://www.amazon.science/scholars" target="_blank" rel="noopener">Amazon</a>
                </div>

                 <div class="profile-contact">
                   <p class="profile-contact-text">manling.li <span>AT northwestern.edu</span></p>
                 </div>
           
                 <ul class="network-icon" aria-hidden="true">
                   
                   
                   
                   
                     
                   
                   
                   
                   
                   
                   <li>
                     <a href="mailto:manling.li@northwestern.edu" >
                       <i class="fas fa-envelope big-icon"></i>
                     </a>
                   </li>
                   
                   
                   
                   
                   
                   
                   
                   
                     
                   
                   <li>
                     <a href="https://scholar.google.com/citations?user=6U4SXnUAAAAJ&amp;hl=en" target="_blank" rel="noopener">
                       <i class="ai ai-google-scholar big-icon"></i>
                     </a>
                   </li>
                   
                   
                   
                   
                     
                   
                   
                   
                   
                   
                     
                   
                   <li>
                     <a href="https://www.linkedin.com/in/manling-li-cs/" target="_blank" rel="noopener">
                       <i class="fab fa-linkedin big-icon"></i>
                     </a>
                   </li>
                   
                   
                   
                   
                     
                   
                   
                   
                   
                   
                     
                   
                   <li>
                     <a href="https://twitter.com/ManlingLi_" target="_blank" rel="noopener">
                       <i class="fab fa-twitter big-icon"></i>
                     </a>
                   </li>
                   
                 </ul>
           
               </div>
      </div>
      <!-- SHARED SIDEBAR END -->

        <div class="col-12 col-lg-9 site-content">
    
    
    

    







  



              
              <div class="universal-wrapper">
                  <h1>Research Statement</h1>
                  <div class="author-name">Manling Li</div>
                  
                  <div class="article-style">
                      <h2>Architecting the Reasoning Interface for Embodied Foundation Agents</h2>
                      
                      <div class="research-vision">
                          <p><strong>My research vision is to fundamentally transform artificial intelligence models from passive observers into active, embodied agents.</strong> Today's foundation models, trained on static web-scale data, excel at semantic-centric reasoning, yet lack grounded interactions with the dynamic and partially observable physical world. For example, they fail at a task a young child masters: when instructed to "put the turkey on the table", they place the cooked turkey directly on the table, without thinking of using plates. This failure captures the core of my research: the <strong>reasoning gap between semantic-centric priors</strong> (turkey and table frequently co-occur in language) and <strong>embodied reasoning</strong> (a turkey requires a plate).</p>
                          
                          <p>I strive to architect this foundational bridge, which I term the <strong>Reasoning Interface</strong>, as a trainable structured reasoning protocol that connects foundation models to physical environments.</p>
                      </div>
                      
                      <div class="section-container">
                          <h3>Core Research Contributions</h3>
                          <p>My solution addresses the core challenges in perception, policy and alignment:</p>
                          
                          <div class="key-contributions">
                              <ul>
                                  <li><strong>Seeing the unseen:</strong> My work was among the first to expose the limitations of geometric reasoning in VLMs [28, 1], and develop Spatial-Geometric Reasoning about 3D spatial geometry [26, 9] and 4D dynamics [6, 33] from partial, egocentric observations.</li>
                                  
                                  <li><strong>Exploring the world:</strong> My work designs Self-Evolving Policy Reasoning by enabling world-model exploration [24, 3, 25] and robust policy learning [30, 8] under sparse feedback, serving as the core engine for generalist embodied agents [23, 32, 25].</li>
                                  
                                  <li><strong>Steering the reasoning:</strong> My work pioneered both mechanism interpretability in spatial intelligence [4, 2] (first to open black box of VLMs and diagnose why they fail in spatial tasks) and cross-modal active intervention [34, 5, 35] (first to dynamically steer multimodal reasoning toward safe and aligned decisions) to build the Mechanistic Control Interface, ensuring trustworthy reasoning [19, 17, 10].</li>
                              </ul>
                          </div>
                      </div>
                      
                      <div class="figure-caption">
                          <em>Figure 1: My research transforms passive AI models to active embodied agents via trainable reasoning interfaces.</em>
                      </div>
                      
                      <div class="section-container">
                          <h3>Research Impact and Recognition</h3>
                          <div class="awards-section">
                              <p>My research probes the intersection between <span class="research-area">Embodied AI</span> <span class="research-area">Natural Language Processing</span> and <span class="research-area">Computer Vision</span>, recognized with honors such as:</p>
                              <ul>
                                  <li>ACL 2025 Inaugural Dissertation Award Honorable Mention (only awardee from the class of 2023)</li>
                                  <li>ACL 2024 Outstanding Paper</li>
                                  <li>NAACL 2021 Best Demo Paper</li>
                                  <li>ACL 2020 Best Demo Paper</li>
                                  <li>Oral and Spotlight presentations at NeurIPS and ICML</li>
                                  <li>Best Paper Awards at ICCV and RSS workshops</li>
                                  <li>MIT Technology Review's Innovators Under 35 Global List</li>
                              </ul>
                              
                              <p>I led the development of open-source multimodal knowledge reasoning system, ranked 1st in NIST SM-KBP evaluation in both 2019 and 2020. My work has been transferred to DARPA and Army Research Lab, and well recognized by the government (DARPA Riser with a featured presentation at the DARPA Forward Event 2022) and industry (Microsoft Research PhD Fellowship and Postdoc Fellowship).</p>
                              
                              <p>I lead the series of tutorials/workshops/challenges on <strong>Foundation Models Meet Embodied Agents</strong> [12], which attracted 300+ conference members at AAAI 2025, NAACL 2025, CVPR 2025, ICCV 2025, and NeurIPS 2025.</p>
                          </div>
                      </div>
                      
                      <div class="section-container">
                          <h2>The Spatial Reasoning Interface: From Semantic-Centric Priors to Spatial-Geometric Reasoning</h2>
                          
                          <p>Foundation models are pre-trained on the semantics of language and static 2D images rather than the geometry of the physical world. This creates a critical reasoning gap: models memorize semantic priors (e.g., "cup is on table") but remain blind to the underlying 3D spatial and temporal-causal structures. They cannot reason about what they have not seen.</p>
                          
                          <p>My <strong>ADAPTVIS</strong> [4] provides the first systematic diagnosis of this failure, showing that current VLMs behave as "style memorizers", attending to linguistically salient but spatially irrelevant regions rather than reasoning on true geometry. It is among first to expose the fundamental weakness in how cross-modal attention encodes spatial relations, motivating the need to integrate reasoning with vision-centric priors.</p>
                          
                          <div class="figure-caption">
                              <em>Figure 2: Spatial Reasoning Interface bridging Semantic-Centric Priors to Spatial-Geometric Reasoning. My work diagnoses VLM reasoning failures and introduces new architectures for 3D/4D spatial and geometric reasoning.</em>
                          </div>
                          
                          <p>To address this, my work introduces a cognitive map based reasoning interface that closes the gap between perception and VLMs. <strong>MindCube</strong> [26] (Best Paper Award at ICCV 2025 Workshop) demonstrates that true spatial intelligence requires an internal Spatial Mental Model. By reconstructing a top-down cognitive map from limited egocentric views, the model learns to reason about unseen perspectives and spatial relations.</p>
                          
                          <p>With these core architectures, my research expands the spatial reasoning interface along new dimensions of scale and modality:</p>
                          <ul>
                              <li><strong>LayoutVLM</strong> [9] optimizes 3D spatial layouts through differentiable vision–language reasoning</li>
                              <li><strong>IKEA Manuals at Work</strong> [6] grounds 4D assembly instructions on real Internet videos</li>
                              <li><strong>HourVideo</strong> [1] scales this paradigm to long-form temporal reasoning across hours of video</li>
                              <li><strong>T*</strong> [33] explores temporal search as an additional dimension of spatial reasoning, unifying spatial, temporal, and semantic grounding within a single framework</li>
                          </ul>
                          
                          <p>My work on Spatial Reasoning enables foundation agents to perceive and reason about the physical world, and has been invited as keynote speech at the first Spatial Intelligence workshop at ICCV 2025.</p>
                      </div>
                      
                      <div class="section-container">
                          <h2>The Self-Evolving Policy Reasoning Interface: From Exploitation to Exploration via World Modeling</h2>
                          
                          <p>My research on policy reasoning addresses a central challenge in embodied AI: adapting foundation models that were pre-trained on static, language-centric data to learn from sparse, delayed, and stochastic reinforcement signals. Standard reinforcement learning usually collapses under this out-of-distribution (OOD) environments.</p>
                          
                          <p>My work <strong>RAGEN</strong> [30] (Best Poster Award at MMLS 2025; 2.3k+ GitHub Stars) was the first to identify the key of this failure mode, which I call the <strong>Echo Trap</strong>. In this state, the agent's reasoning collapses into repetitive, self-reinforcing patterns and overfits to familiar thought trajectories instead of exploring new strategies. RAGEN establishes the foundation for stable and robust reinforcement learning with reasoning agents, with a Markov Decision Process (MDP) grounded <strong>State–Thinking–Action–Reward Policy Optimization (StarPO)</strong>. It later was widely adopted as the foundation for multi-turn RL with follow ups such as Search-R1, Agent-R1, etc.</p>
                          
                          <div class="figure-caption">
                              <em>Figure 3: Policy Reasoning Interface with our State–Thinking–Action–Reward Optimization and World Modeling.</em>
</div>

                          <p>To encourage exploration over exploitation, we propose <strong>VAGEN</strong> [24] to learn an internal world model that grounds reinforcement learning in structured belief states to stabilize policy reasoning under partial observability (POMDP). VAGEN has been able to successfully support various environments such as robotic manipulation, games, and SVG figures.</p>
                          
                          <p>We further explore where is the best to inject world model knowledge in agentic RL:</p>
                          <ul>
                              <li>My <strong>Self-Play Agent</strong> [3] encourages foundation models to learn to self-play with the environment to learn world models</li>
                              <li><strong>ROSETTA</strong> [8] (Best Paper Award at RSS 2025 workshop) further learns from human feedback with language-grounded reward modeling</li>
                              <li><strong>ENACT</strong> [25] further expands this reasoning interface toward multi-step world modeling</li>
                          </ul>
                          
                          <p>Our <strong>Embodied Agent Interface</strong> [23] (Oral Top 0.4% at NeurIPS, Best Paper at SoCal NLP 2024) and <strong>EmbodiedBench</strong> [32] (Oral Top 1% at ICML 2025) further serve as the core engine to provide feedback for embodied agents.</p>
                          
                          <p>Collectively, this line of research redefines the policy reasoning interface and transforms reinforcement learning from a low-level optimization procedure into a high-level reasoning framework that guides foundation models toward exploration, adaptation, and self-evolution.</p>
  </div>
    
                      <div class="section-container">
                          <h2>The Mechanistic Control Interface: From Black Boxes to Safe Agents</h2>
    
                          <p>My research on mechanistic control tackles the final challenge in building reasoning agents: transforming black-box models into safe, interpretable, and steerable systems. As foundation models scale, their reasoning processes become opaque, which makes it difficult to diagnose errors or enforce alignment. The Mechanistic Control Interface bridges this gap by enabling agents to reason not only about the external world but also about their own internal mechanisms.</p>
                          
                          <div class="figure-caption">
                              <em>Figure 4: I work towards safe and controllable foundation models via improving mechanistic interpretability.</em>
</div>

                          <p>The first step toward this goal is to improve the <strong>reasoning controllability</strong>. <strong>LM-Steer</strong> [5] (ACL 2024, Outstanding Paper Award) and <strong>ODESteer</strong> [35] introduce a direct steering interface that manipulates activation vectors at inference time, providing fine-grained control over safety, style, and factuality without retraining. <strong>HallE-Control</strong> [34] is the first to control visual hallucination in multimodal models.</p>
  
                          <p><strong>Interpretability</strong> is well supported by this interface. My work to answer why is spatial reasoning hard [4] (ICML 2025) opens up VLMs analyzing attention mechanisms, the first to reveal the roots of spatial reasoning failure. <strong>Diffusion Transformer Grafting</strong> [2] (Oral Top 0.36% at NeurIPS 2025) shows how architectural components shape reasoning behavior by editing pretrained models.</p>
    
                          <p><strong>Trustworthiness</strong> is centered my research from the very beginning to ensure that reasoning remains grounded in verifiable knowledge. My doctoral thesis on event-centric multimodal knowledge acquisition [10] (ACL Inaugural Best Dissertation Award Honorable Mention) and <strong>CLIP-Event</strong> [18] (Oral at CVPR 2022) established event-centric multimodal grounding as a foundation for factual reasoning. With this foundation, my <strong>GAIA system</strong> [19, 15, 14] and my <strong>COVID-19 Claim Radar</strong> [17], demonstrate factual reliability at scale and achieve first place at the NIST SM-KBP evaluations in both 2019 and 2020.</p>

                          <p>Through these efforts, the Mechanistic Control Interface transforms agents from black boxes into safe, interpretable systems, closing the final gap between intelligent behavior and reliable alignment.</p>
                      </div>

                      <div class="future-research">
                          <h2>Future Research</h2>

                          <p>My future research will extend the frontier of embodied and interpretable intelligence:</p>

                          <h3>Mechanistic interpretability of 3D-grounded VLMs and VLAs</h3>
                          <p>A key next step is to open the black box of 3D-aware foundation models and VLAs and understand how they internalize physics and affordances, probing how models trained on dynamic visual data form internal "physics circuits". I plan to study whether subnetworks or attention heads are causally responsible for predicting gravity, collision, and containment, and how concepts such as "graspable" emerge separately from visual identity. Activation patching and causal editing will allow us to test whether we can transfer the "is-graspable" property between objects or locate "pushing" circuits that map actions to predicted 3D outcomes. Through probing for geometric primitives, such as coordinates, orientation, and occlusion states, I will characterize how 3D world models are represented and manipulated within large vision–language systems.</p>

                          <h3>From passive to active exploration like humans</h3>
                          <p>I aim to develop agents that represent what is known about a scene, quantify what remains unknown, and use that uncertainty to guide exploration [24, 25]. This involves building an internal belief state that integrates both geometric and semantic layers: a geometric representation such as in 3D space, overlaid with a semantic scene graph encoding abstraction of entities and relations. The agent will actively seek keyframes that reduce uncertainty and fuse new observations back into the belief state, transforming passive perception into curiosity-driven reasoning.</p>

                          <h3>Representation learning for space to create generalist embodied agents with spatial priors</h3>
                          <p>My research will focus on a new training paradigm to solve a critical flaw in current VLAs: fine-tuning for expert action often causes forgetting of the VLM's generalist language and reasoning priors. I aim to answer how do we best distill and collect spatial-geometric priors, and what is the optimal representation for space? This approach will use geometric consistency to stabilize learning in low-data regimes, and also will produce a unified agent that can act on, reason about, and explain its interactions with the physical world, closing the loop between perception, reasoning, and control.</p>

                          <h3>Certifying the safety of embodied agents through control theory</h3>
                          <p>I plan to extend my previous work on Control Barrier Function into formal verification of embodied systems. This line of research aims to make embodied reasoning not only powerful but also provably safe. I will identify activation patterns that correspond to unsafe behaviors, design monitors that track them in real time, and implement intervention layers that can override dangerous trajectories. This CBF-driven study will redefine alignment as a mechanistic process that can be achieved through precise and interpretable control of internal activations.</p>
                      </div>

                      <div class="references">
                          <h2>References</h2>
                          <div class="reference-item">[1] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. HourVideo: 1-Hour Video-Language Understanding. In NeurIPS D&B Track, 2024.</div>
                          
                          <div class="reference-item">[2] Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, and Fei-Fei Li. Exploring Diffusion Transformer Designs via Grafting. In NeurIPS Oral (Oral Presentation (Top 0.36%)), 2025.</div>
                          
                          <div class="reference-item">[3] Shiqi Chen, Tongyao Zhu, Zian Wang, Jinghan Zhang, Kangrui Wang, Siyang Gao, Teng Xiao, Yee Whye Teh, Junxian He, and Manling Li. Internalizing World Models via Self-Play Finetuning for Agentic RL. In Submission, 2025.</div>
                          
                          <div class="reference-item">[4] Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, and Manling Li. Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas. In ICML, 2025.</div>
                          
                          <div class="reference-item">[5] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, and Heng Ji. Word Embeddings Are Steers for Language Models. In ACL, 2024. Outstanding Paper Award at ACL 2024.</div>
                          
                          <div class="reference-item">[6] Yunong Liu, Cristobal Eyzaguirre, Manling Li, Shubh Khanna, Juan Carlos Niebles, Vineeth Ravi, Saumitra Mishra, Weiyu Liu, and Jiajun Wu. IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos. In NeurIPS D&B Track, 2024.</div>
                          
                          <div class="reference-item">[7] Tengfei Ma, Manling Li, Mo Yu, Gao Tian, and Achille Fokoue. Unsupervised Knowledge Graph Compression based on Optimal Transport. In U.S. Patent US20220335270A1, 2022.</div>
                          
                          <div class="reference-item">[8] Sanjana Srivastava*, Kangrui Wang*, Yung-Chieh Chan*, Tianyuan Dai, Manling Li, Ruohan Zhang, Mengdi Xu, Jiajun Wu, and Li Fei-Fei. ROSETTA: Constructing Code-Based Reward from Unconstrained Language Preference. In Submission, 2025. Best Paper Award at RSS 2025 workshop on Continual Robot Learning from Humans.</div>
                          
                          <div class="reference-item">[9] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models. In CVPR, 2025.</div>
                          
                          <div class="reference-item">[10] Manling Li. Event-centric multimodal knowledge acquisition. PhD thesis, University of Illinois at Urbana-Champaign, 2023. ACL Inaugural Best Dissertation Award Honorable Mention.</div>
                          
                          <!-- Additional references continue in same format... -->
                          <div class="reference-item">[30] Zihan Wang*, Kangrui Wang*, Qineng Wang*, Pingyue Zhang*, Linjie Li*, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning. In Submission, 2025. 2.4k Github Stars as an Open-Source RL Agent framework. Best Poster Award at MMLS 2025 (Midwest Machine Learning Symposium).</div>
                          
                          <p style="text-align: center; margin-top: 20px; font-size: 0.9rem; color: #666;">
                              <em>Note: Complete reference list abbreviated for brevity. See full document for all 35 references.</em>
                          </p>
      </div>
                  </div>
              </div>
  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Fun Projects</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="article-style">

    <p><strong>Fun Projects</strong><br></p>

    <p><strong>Architecting the Reasoning Interface for Embodied Foundation Agents</strong></p>

    <p>My research vision is to fundamentally transform artificial intelligence models from passive observers
    into active, embodied agents. Today’s foundation models, trained on static web-scale data, excel at
    semantic-centric reasoning, yet lack grounded interactions with the dynamic and partially observable
    physical world. For example, they fail at a task a young child masters: when instructed to “put the turkey
    on the table”, they place the cooked turkey directly on the table, without thinking of using plates. This
    failure captures the core of my research: the reasoning gap between semantic-centric priors (turkey and
    table frequently co-occur in language) and embodied reasoning (a turkey requires a plate) .</p>

    <p>I strive to architect this foundational bridge, which I term the Reasoning Interface, as a trainable
    structured reasoning protocol that connects foundation models to physical environments. My solution
    addresses the core challenges in perception, policy and alignment:</p>

    <ul>
      <li>Seeing the unseen: My work was among the first to expose the limitations of geometric reasoning
      in VLMs [28, 1], and delveop Spatial-Geometric Reasoning about 3D spatial geometry [26, 9] and 4D
      dynamics [6, 33] from partial, egocentric observations.</li>
      <li>Exploring the world: My work designs Self-Evolving Policy Reasoning by enabling world-model
      exploration [24, 3, 25] and robust policy learning [30, 8] under sparse feedback, serving as the core
      engine for generalist embodied agents [23, 32, 25].</li>
      <li>Steering the reasoning: My work pioneered both mechanisium interpretability in spatial intelli­
      gence [4, 2] (first to open black box of VLMs and diagnose why they fail in spatial tasks) and cross-modal
      active intervention [34, 5, 35] (first to dynamically steer multimodal reasoning toward safe and aligned
      decisions) to build the Mechanistic Control Interface, ensuring trustworthy reasoning [19, 17, 10].</li>
    </ul>

    <p><em>Figure 1: My research transforms passive AI models to active embodied agents via trainable reasoning interfaces.</em></p>


    <p>1https://foundation-models-meet-embodied-agents.github.io</p>

    <p><strong>The Spatial Reasoning Interface: From Semantic-Centric Priors to Spatial-Geometric Reasoning</strong></p>

    <p>Foundation models are pre-trained on the semantics of language and static 2D images rather than the
    geometry of the physical world. This creates a critical reasoning gap: models memorize semantic priors
    (e.g., “cup is on table”) but remain blind to the underlying 3D spatial and temporal-causal structures. They
    cannot reason about what they have not seen. My ADAPTVIS [4] provides the first systematic diagnosis of
    this failure, showing that current VLMs behave as “style memorizers”, attending to linguistically salient
    but spatially irrelevant regions rather than reasoning on true geometry. It is among first to expose the
    fundamental weakness in how cross-modal attention encodes spatial relations, motivating the need to
    integrate reasoning with vision-centric priors.</p>

    <p><em>Figure 2: Spatial Reasoning Interface bridging Semantic-Centric Priors to Spatial-Geometric Reasoning. My work
    diagnoses VLM reasoning failures and introduces new architectures for 3D/4D spatial and geometric reasoning.</em></p>

    <p>To address this, my work introduces a cognitive map based reasoning interface that close the gap between
    perception and VLMs. MindCube [26] (Best Paper Award at ICCV 2025 Workshop) demonstrates that true
    spatial intelligence requires an internal Spatial Mental Model. By reconstructing a top-down cognitive map
    from limited egocentric views, the model learns to reason about unseen perspectives and spatial relations.
    With these core architectures, my research expand the spatial reasoning interface along new dimensions of
    scale and modality. LayoutVLM [9] optimizes 3D spatial layouts through differentiable vision–language
    reasoning, IKEA Manuals at Work [6] grounds 4D assembly instructions on real Internet videos, and
    HourVideo [1] scales this paradigm to long-form temporal reasoning across hours of video. T* [33]
    explores temporal search as an additional dimension of spatial reasoning, unifying spatial, temporal,
    and semantic grounding within a single framework. My work on Spatial Reasoning enables foundation
    agents to perceive and reason about the physical world, and has been invited as keynote speech at the
    first Spatial Intelligence workshop at ICCV 2025.</p>

    <p><strong>The Self-Evolving Policy Reasoning Interface: From Exploitation to Exploration via World Modeling</strong></p>

    <p>My research on policy reasoning addresses a central challenge in embodied AI: adapting foundation
    models that were pre-trained on static, language-centric data to learn from sparse, delayed, and stochastic
    reinforcement signals. Standard reinforcement learning usually collapses under this out-of-distribution
    (OOD) environments. My work RAGEN [30] (Best Poster Award at MMLS 2025; 2.3k+ GitHub Stars) was
    the first to identify the key of this failure mode, which I call the Echo Trap. In this state, the agent’s reasoning
    collapses into repetitive, self-reinforcing patterns and overfits to familiar thought trajectories instead of
    exploring new strategies. RAGEN establishes the foundation for stable and robust reinforcement learning
    with reasoning agents, with a Markov Decision Process (MDP) grounded State–Thinking–Action–Reward
    Policy Optimization (StarPO). It later was widely adopted as the foundation for multi-turn RL with follow
    ups such as Search-R1, Agent-R1, etc.</p>

    <p><em>Figure 3: Policy Reasoning Interface with our State–Thinking–Action–Reward Optimization and World Modeling.</em></p>

    <p>To encourage exploration over exploitation, we propose VAGEN [24] to learn an internal world model
    that grounds reinforcement learning in structured belief states to stabilize policy reasoning under partial
    observability (POMDP). VAGEN has been able to successfully support various enviornments such as
    robotic manipulation, games, and SVG figures. We further explore where is the best to inject world
    model knowledge in agentic RL: my Self-Play Agent [3] encourage foundation models to learn to
    self-play with the environemnt to learn to world models. ROSETTA [8] (Best Paper Award at a RSS
    2025 workshop) further learn from human feedback with language-grounded reward modeling, and
    ENACT [25] further expand this reasoning interface toward multi-step world modeling. Our Embodied
    Agent Interface [23] (Oral Top 0.4% at NeurIPS, Best Paper at SoCal NLP 2024) and EmbodiedBench [32]
    (Oral Top 1% at ICML 2025) further serve as the core engine to provide feedback for embodied agents.
    Collectively, this line of research redefines the policy reasoning interface and transforms reinforcement
    learning from a low-level optimization procedure into a high-level reasoning framework that guides
    foundation models toward exploration, adaptation, and self-evolution.</p>

    <p><strong>The Mechanistic Control Interface: From Black Boxes to Safe Agents</strong></p>

    <p>My research on mechanistic control tackles the final challenge in building reasoning agents: transforming
    black-box models into safe, interpretable, and steerable systems. As foundation models scale, their
    reasoning processes become opaque, which makes it difficult to diagnose errors or enforce alignment. The
    Mechanistic Control Interface bridges this gap by enabling agents to reason not only about the external
    world but also about their own internal mechanisms.</p>

    <p><em>Figure 4: I work towards safe and controllable foundation models via improving mechanistic interpretability.</em></p>

    <p>The first step toward this goal is to improve the reasoning controllability. LM-Steer [5] (ACL 2024,
    Outstanding Paper Award) and ODESteer [35] introduce a direct steering interface that manipulates
    activation vectors at inference time, providing fine-grained control over safety, style, and factuality
    without retraining. HallE-Control [34] is the first to control visual hallucination in multimodal models.
    Interpretability is well supported by this interface. My work to answer why is spatial reasoning hard [4]
    (ICML 2025) opens up VLMs analyzing attention mechanisms, the first to reveal the roots of spatial
    reasoning failure. Diffusion Transformer Grafting [2] (Oral Top 0.36% at NeurIPS 2025) shows how
    architectural components shape reasoning behavior by editing pretrained models. Trustworthiness is
    centered my research from the very begining to ensure that reasoning remains grounded in verifiable
    knowledge. My doctoral thesis on event-centric multimodal knowledge acquisition [10] (ACL Inaugural
    Best Dissertation Award Honorable Mention) and CLIP-Event [18] (Oral at CVPR 2022) established
    event-centric multimodal grounding as a foundation for factual reasoning. With this foundation, my
    GAIA system [19, 15, 14] and my COVID-19 Claim Radar [17], demonstrate factual reliability at scale
    and achieve first place at the NIST SM-KBP evaluations in both 2019 and 2020. Through these efforts,
    the Mechanistic Control Interface transforms agents from black boxes into safe, interpretable systems.
    closing the final gap between intelligent behavior and reliable alignment.</p>

    <p><strong>Future Research</strong></p>

    <p>My future research will extend the frontier of embodied and interpretable intelligence:
    Mechanistic interpretability of 3D-grounded VLMs and VLAs. A key next step is to open the black box
    of 3D-aware foundation models and VLAs and understand how they internalize physics and affordances,
    probing how models trained on dynamic visual data form internal “physics circuits”. I plan to study
    whether subnetworks or attention heads are causally responsible for predicting gravity, collision, and
    containment, and how concepts such as “graspable” emerge separately from visual identity. Activation
    patching and causal editing will allow us to test whether we can transfer the “is-graspable” property
    between objects or locate “pushing” circuits that map actions to predicted 3D outcomes. Through probing
    for geometric primitives, such as coordinates, orientation, and occlusion states, I will characterize how 3D
    world models are represented and manipulated within large vision–language systems.</p>

    <p>From passive to active exploration like humans. I aim to develop agents that represent what is known
    about a scene, quantify what remains unknown, and use that uncertainty to guide exploration [24, 25]. This
    involves building an internal belief state that integrates both geometric and semantic layers: a geometric
    representation such as in 3D space, overlaid with a semantic scene graph encoding abstraction of entities
    and relations. The agent will actively seek keyframes that reduce uncertainty and fuse new observations
    back into the belief state, transforming passive perception into curiosity-driven reasoning.</p>

    <p>Representation learning for space to create generalist embodied agents with spatial priors. My
    research will focus on a new training paradigm to solve a critical flaw in current VLAs: fine-tuning for
    expert action often causes forgetting of the VLM’s generalist language and reasoning priors. I aim to
    answer how do we best distill and collect spatial-geometric priors, and what is the optimal representation
    for space? This approach will use geometric consistency to stabilize learning in low-data regimes, and also
    will produce a unified agent that can act on, reason about, and explain its interactions with the physical
    world, closing the loop between perception, reasoning, and control.</p>

    <p>Certifying the safety of embodied agents through control theory. I plan to extend my previous work
    on Control Barrier Functioninto formal verification of embodied systems. This line of research aims to
    make embodied reasoning not only powerful but also provably safe. I will identify activation patterns that
    correspond to unsafe behaviors, design monitors that track them in real time, and implement intervention
    layers that can override dangerous trajectories. This CBF-driven study will redefine alignment as a
    mechanistic process that can be achieved through precise and interpretable control of internal activations.</p>
    
  </div>
</div>

  

  

  

        </div>
      </div>
</div>
  </div>

      <div class="page-footer">
  <footer class="site-footer" role="contentinfo">
    <div class="footer-inner">
      <p class="footer-title">Copyright © 2025 Manling Li</p>
  </footer>
</div>



  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
