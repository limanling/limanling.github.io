<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: February 8, 2025 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.1ad9d7fd94c7e3d373fab9f28756eb4f.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  























  
  
  






  <meta name="author" content="Manling Li" />





  

<meta name="description" content="Manling Li" />



<link rel="alternate" hreflang="en-us" href="https://limanling.github.io/" />
<link rel="canonical" href="https://limanling.github.io/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu437e05b1b543de801281f81b261d0785_1145_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu437e05b1b543de801281f81b261d0785_1145_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@ManlingLi_" />
  <meta property="twitter:creator" content="@ManlingLi_" />
<meta property="twitter:image" content="https://limanling.github.io/media/icon_hu437e05b1b543de801281f81b261d0785_1145_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Manling Li" />
<meta property="og:url" content="https://limanling.github.io/" />
<meta property="og:title" content="Manling Li" />
<meta property="og:description" content="Manling Li" /><meta property="og:image" content="https://limanling.github.io/media/icon_hu437e05b1b543de801281f81b261d0785_1145_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta property="og:updated_time" content="2022-10-24T00:00:00&#43;00:00" />
  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://limanling.github.io?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "https://limanling.github.io"
}
</script>


  




  
  
  
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  

  
  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Manling Li" />
  

  


  
  <title>Manling Li</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   " data-wc-page-id="3976528693a0108357f4928017600865" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Manling Li</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Manling Li</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#news" data-target="#news"><span>News</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications" data-target="#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#services" data-target="#services"><span>Services</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#teaching" data-target="#teaching"><span>Teaching</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#students" data-target="#students"><span>Students</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    


  









  

<span class="js-widget-page d-none"></span>



  
































  
  


























<section id="about" class="home-section wg-aboutme  " style="padding: 0 0 0 0;" >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    


 
 
 
   
 
 
 
 
 
 
 
 
 
 
 <div class="row">
   <div class="col-12 col-lg-4">
     <div id="profile">
 
       
       
       <img class="avatar avatar-circle" src="/authors/admin/avatar_hudd2cb509b9271810998b42a25e34ae22_4743348_270x270_fill_q75_lanczos_center.jpg" alt="Avatar">
       
 
       <div class="portrait-title">
         <h2></h2>
         <h3>Assistant Professor</h3>
 
         
         <h3>
           <a href="https://www.mccormick.northwestern.edu/computer-science/" target="_blank" rel="noopener">
           <span>Computer Science Department</span>
           </a>
         </h3>
         
         <h3>
           <a href="https://www.northwestern.edu/" target="_blank" rel="noopener">
           <span>Northwestern University</span>
           </a>
         </h3>
         
         <h3>
           
           <span>manling.li@northwestern.edu</span>
           
         </h3>
         

         
 
         
       </div>
 
       <ul class="network-icon" aria-hidden="true">
         
         
         
         
           
         
         
         
         
         
         <li>
           <a href="mailto:manling.li@northwestern.edu" >
             <i class="fas fa-envelope big-icon"></i>
           </a>
         </li>
         
         
         
         
         
         
         
         
           
         
         <li>
           <a href="https://scholar.google.com/citations?user=6U4SXnUAAAAJ&amp;hl=en" target="_blank" rel="noopener">
             <i class="ai ai-google-scholar big-icon"></i>
           </a>
         </li>
         
         
         
         
           
         
         
         
         
         
           
         
         <li>
           <a href="https://twitter.com/ManlingLi_" target="_blank" rel="noopener">
             <i class="fab fa-twitter big-icon"></i>
           </a>
         </li>
         
       </ul>
 
     </div>
   </div>
   <div class="col-12 col-lg-8">
 
     
     
 
     <p><!-- In the meantime, I am visiting [Stanford University](http://vision.stanford.edu/), working with Prof [Jiajun Wu](https://jiajunwu.com/), Prof [Fei-Fei Li](https://profiles.stanford.edu/fei-fei-li/), and Prof [Monica Lam](https://oval.cs.stanford.edu).  -->
<p>Hi, there! I&rsquo;m <strong>Manling</strong>. I am an assistant professor at the Computer Science department of <a href="https://www.northwestern.edu/" target="_blank" rel="noopener">Northwestern University</a>.
Prior to this, I was a postdoc and fortunate to be advised by Prof <a href="https://jiajunwu.com/" target="_blank" rel="noopener">Jiajun Wu</a> and mentored by Prof <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank" rel="noopener">Fei-Fei Li</a> with support partially from Microsoft Research Postdoc Fellowship.
I obtained my Ph.D. degree in Computer Science at <a href="https://illinois.edu/" target="_blank" rel="noopener">University of Illinois Urbana-Champaign</a> in 2023. I was fortunate to have Prof <a href="http://blender.cs.illinois.edu/hengji.html" target="_blank" rel="noopener">Heng Ji</a> as my advisor and grateful for the mentorship from Prof <a href="https://www.engineering.columbia.edu/faculty/shih-fu-chang" target="_blank" rel="noopener">Shih-Fu Chang</a>, Prof <a href="https://kyunghyuncho.me/" target="_blank" rel="noopener">Kyunghyun Cho</a> and Prof <a href="https://hanj.cs.illinois.edu/" target="_blank" rel="noopener">Jiawei Han</a>, and for receiving the support from Microsoft Research PhD Fellowship.</p>
<!-- I was a postdoc in 2023-2024, and have the fortune to work with Prof [Jiajun Wu](https://jiajunwu.com/), Prof [Fei-Fei Li](https://profiles.stanford.edu/fei-fei-li/), Prof [], with partial support from Microsoft Research Postdoc Fellowship.  -->
<p>I aim to make AI and LLMs beneficial for humans, working at the intersection of language, vision, robotics, and their impact on society. I focus on Knowledgeable Foundation Models for <strong>Reasoning, Planning, Compositionality</strong>, with applications in Embodied AI and AI for science. At the core of my research, I aim to make machines to interact with <strong>physical world</strong> via <strong>multimodal data (Language + X, where X can be robotics, vision, audio, etc)</strong>. The ultimate goal of my research is to promote <strong>trustworthiness and truthfulness</strong> in through a structured view that is easily explainable, highly compositional, and capable of long-horizon reasoning.</p>
<!-- Prior to UIUC, I got my master's degree from [Institute of Computing Technology](http://www.ict.ac.cn/), [Chinese Academy of Sciences](http://www.cas.cn/) in 2018, and received my bachelor's degree from [University of Science and Technology Beijing](http://www.ustb.edu.cn/index.asp) with Beijing Outstanding Undergraduate Award in 2015. -->
<!-- Her research probes the intersection of Natural Language Processing (NLP) and Computer Vision (CV). She aims to turn text and vision information access **from entity-centric to event-centric**, providing a structured knowledge view that is easily explainable, highly compositional, and capable of reasoning. -->
<!-- What happens? Who? When? Where? Why?* are the fundamental questions asked to comprehend the overwhelming amount of information, regardless of whether presented as text, images, or videos. I propose to evolve traditional Entity-Centric Single-Modalitiy Knowledge to **Event-Centric Multi-Modality Knowledge**.  -->
<!-- My research aims to equip machines with **deep semantic understandings of multimodal information**. 
*What happens? Who? When? Where? Why?* are the fundamental questions asked to comprehend the overwhelming amount of information, regardless of whether presented as text, images, or videos. 
I aim to evolve traditional Entity-Centric Single-Modalitiy Knowledge to **Event-Centric Multi-Modality Knowledge**, including: 
<br>
(1) understanding **multimodal event semantic structures** to answer *What happened?, Who?, Where?, and When?* (Knowledge Extraction):  We bring event argument (participant) semantics into vision-language pretraining (CLIP-Event), which has supported zero-shot joint Multimedia Event Extraction (M<sup>2</sup>E<sup>2</sup> for the first time;
<br> 
(2) understanding **temporal dynamics** to answer *What will happen next? and Why?* (Knowledge Reasoning): We introduce Event Graph Schema, which opens doors to a global event graph context capturing participant-specific and temporally ordered event information; 
<br>
(3) **applying Event Knowledge Graph** (Knowledge-Driven Applications): We show positive results on long-standing open problems, such as timeline generation, meeting summarization, question answering. -->
<p><strong>Bio:</strong>
Her work on multimodal knowledge extraction won the <strong>ACL'20 Best Demo Paper Award</strong>, the work on scientific information extraction from COVID literature won <strong>NAACL'21 Best Demo Paper Award</strong>, the work on controlling LLMs (LM-Steer) won <strong>ACL'24 Outstanding Paper Award</strong>. She was a recipient of <strong>Microsoft Research PhD Fellowship</strong> in 2021. She was selected as a <strong>DARPA Riser</strong> in 2022 (nominated by DARPA), and a <strong>EE CS Rising Star</strong> in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation each phase. She gave tutorials about multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, CVPR'23, IJCAI'24, and serves as Organizing Committee of ACL'25, NAACL'25, EMNLP'24, etc</p>
<!-- She serves as a senior PC member at IJCAI and PC members for ARR, ACL, EMNLP, NAACL, COLING, IJCAI, AAAI, etc.  -->
<!-- <span style="color:blue"><b>I will join [CS @ Northwestern](https://www.mccormick.northwestern.edu/computer-science/) as an assistant professor in Fall 2024. Before that, I will be a postdoc working on multimodal research at [Stanford Vision and Learning Lab](http://vision.stanford.edu/) with Prof. [Jiajun Wu](https://jiajunwu.com/). </b></span><br> -->
<!-- <i class="fas fa-download"></i> Download my [CV](uploads/cv_limanling.pdf),   [ResearchStatement](uploads/ResearchStatement_ManlingLi.pdf),  [TeachingStatement](uploads/TeachingStatement_ManlingLi.pdf),  [DiversityStatement](uploads/DiversityStatement_ManlingLi.pdf)<br> -->
<p><i class="fas fa-envelope"></i> Prospective students: I have 3-5 PhD positions and 1 postdoc position in Fall 2025 or later. Please apply to <a href="https://www.mccormick.northwestern.edu/computer-science/academics/graduate/admissions" target="_blank" rel="noopener">NorthwesternCS</a>. Due to the large amount of emails, <b>I aplogize that I will not able to reply to individual emails</b>. Please <b>choose me as the potential advisor in the application</b>, and I will <b>check every application carefully</b> in late Dec and do interviews in Jan-Mar. <a href="https://hackmd.io/@zAmnkcJDS5CHhVb-PzcMPA/S15pslRn0" target="_blank" rel="noopener">Prospective_Students_English</a>, <a href="https://www.1point3acres.com/bbs/thread-1084711-1-1.html" target="_blank" rel="noopener">Prospective_Students_Chinese</a></p>
<!-- (https://hackmd.io/@zAmnkcJDS5CHhVb-PzcMPA/HyJyoQuhR) -->
<p><i class="fas fa-envelope"></i> I have positions for remote/in-person interns and visiting students. Please submit this <a href="https://forms.gle/unVqpW97bGwQsrRz5" target="_blank" rel="noopener">form</a> and drop me an email <a href="mailto:manling.li@northwestern.edu">manling.li@northwestern.edu</a> with the title &ldquo;Prospective Intern/Visiting Student - YourName&rdquo; if you are interested. Thanks!</p>
<!-- <i class="fas fa-envelope"></i> Prospective students: Northwestern CS has a PhD Application Feedback Program, which aims to help first-generation and underrepresented students with their applications to CS PhD programs in the US. An advanced PhD student, postdoctoral fellow, or CS faculty volunteer will provide one round of feedback on an applicant's resume and statement of purpose (capacity limited). To participate in this year's PhD Application Feedback Program, please submit your application materials by November 1st. Please find details [here](https://www.mccormick.northwestern.edu/computer-science/academics/graduate/admissions).  -->
<!-- Please feel free to drop me an email [manling.li@northwestern.edu](mailto:manling.li@northwestern.edu)  with the title "Prospective Student/Postdoc - NorthwesternCS - YourName". <b> -->
<p><i class="fa fa-microphone"></i> Junior PhD/master/undergraduate students: I will dedicate 30 minutes each week to offer guidance/suggestions/mentorship, especially for students from underrepresented groups or whoever is in need. If you would like to chat about life, career path, graduate school applications, or research ideas related to AI/ML, feel free to file the <a href="https://forms.gle/f7adt4tFJGvn2Dpb7" target="_blank" rel="noopener">form</a> to schedule a meeting.</p>
</p>
 
     <div class="row">

      
      
 
       
       
       
       <div class="col-md-12">
         <h3>Interests</h3>
         <ul class="ul-interests">
           
           <li>Language + Embodied AI <a href="https://embodied-agent-interface.github.io" target="_blank" rel="noopener">Embodied Agent Interface</a>, <a href="https://yunongliu1.github.io/ikea-video-manual/" target="_blank" rel="noopener">IKEA Manual at Work</a>, <a href="https://aclanthology.org/2023.acl-long.303.pdf" target="_blank" rel="noopener">Planning Logic</a>, <a href="https://aclanthology.org/2023.findings-acl.122.pdf" target="_blank" rel="noopener">Procedural Planning</a>,  <a href="https://aclanthology.org/2023.findings-acl.63.pdf" target="_blank" rel="noopener">Agriculture Task Planning</a></li>
           
           <li>Language + Image <a href="https://mikewangwzhl.github.io/VDLM/" target="_blank" rel="noopener">Visually Descriptive Language Modeling</a>, <a href="https://arxiv.org/pdf/2310.01779.pdf" target="_blank" rel="noopener">Object Hallucination</a>, <a href="https://blender.cs.illinois.edu/tutorial/knowledgeVLP/" target="_blank" rel="noopener">Knowledge-Driven Vision-Language Pretraining</a>, <a href="https://aclanthology.org/2023.emnlp-main.824/" target="_blank" rel="noopener">Image2Code</a>, <a href="https://nips.cc/virtual/2023/poster/70715" target="_blank" rel="noopener">VisualKnowledge</a>, <a href="https://arxiv.org/abs/2210.04287" target="_blank" rel="noopener">VisualPrompt</a>, <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf" target="_blank" rel="noopener">CLIP-Event</a>, <a href="https://arxiv.org/abs/2112.10728" target="_blank" rel="noopener">MuMuQA</a>, <a href="https://aclanthology.org/2020.acl-main.230/" target="_blank" rel="noopener">M2E2</a></li>
           
           <li>Language + Video <a href="">HourVideo</a>, <a href="https://arxiv.org/abs/2205.10747" target="_blank" rel="noopener">LM4Video</a>, <a href="https://arxiv.org/abs/2211.01781" target="_blank" rel="noopener">VideoArgument</a>, <a href="https://arxiv.org/abs/2109.12776" target="_blank" rel="noopener">VideoEvent</a></li>
           
           <li>Reasoning, Planning, and Compositionality <a href="https://mikewangwzhl.github.io/VDLM/" target="_blank" rel="noopener">Vector Graphics Reasoning</a>, <a href="https://embodied-agent-interface.github.io" target="_blank" rel="noopener">Embodied Agent Interface</a>, <a href="https://yunongliu1.github.io/ikea-video-manual/" target="_blank" rel="noopener">4D Planning</a>, <a href="https://aclanthology.org/2023.acl-long.312/" target="_blank" rel="noopener">LLM Schema</a>, <a href="https://aclanthology.org/2023.acl-long.303.pdf" target="_blank" rel="noopener">Planning Logic</a>, <a href="https://aclanthology.org/2023.findings-acl.122.pdf" target="_blank" rel="noopener">Procedural Planning</a>,  <a href="https://aclanthology.org/2023.findings-acl.63.pdf" target="_blank" rel="noopener">Agriculture Task Planning</a>, <a href="https://aclanthology.org/2022.naacl-main.147/" target="_blank" rel="noopener">Graph Schema</a>, <a href="https://arxiv.org/abs/2104.06344" target="_blank" rel="noopener">Generative Graph Schema</a>, <a href="https://aclanthology.org/2020.emnlp-main.50/" target="_blank" rel="noopener">PathLM</a>, <a href="https://arxiv.org/abs/2210.04287" target="_blank" rel="noopener">Prompt Decomposition</a></li>
           
           <li>Alignment <a href="https://aclanthology.org/2023.emnlp-main.824/" target="_blank" rel="noopener">Vision-Code</a>, <a href="">Deep Concept Injection</a>, <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf" target="_blank" rel="noopener">CLIP-Event</a>, <a href="https://arxiv.org/abs/2206.02082" target="_blank" rel="noopener">Vision-Language Projection</a>, <a href="https://aclanthology.org/2020.acl-main.230/" target="_blank" rel="noopener">CrossModal Common Semantic Space</a></li>
           
           <li>Mechanistic Interpretability <a href="https://github.com/Glaciohound/LM-Steer" target="_blank" rel="noopener">LM-Steer</a>, <a href="https://arxiv.org/abs/2407.12828" target="_blank" rel="noopener">Ripple Effect</a>, <a href="https://arxiv.org/abs/2407.08039" target="_blank" rel="noopener">Knowledge Overshadow</a></li>
           
           <li>Factuality, Truthfulness, and Social Good <a href="https://arxiv.org/abs/2303.14337" target="_blank" rel="noopener">SmartBook</a>, <a href="https://arxiv.org/abs/2311.15642.pdf" target="_blank" rel="noopener">Info Propagation Pattern</a>, <a href="https://arxiv.org/abs/2402.07401" target="_blank" rel="noopener">Explainable Fact Checking</a>, <a href="https://arxiv.org/pdf/2310.01779" target="_blank" rel="noopener">Image Hallucination</a>, <a href="https://arxiv.org/abs/2211.05414" target="_blank" rel="noopener">DebiasPrompt</a>, <a href="https://aclanthology.org/2021.emnlp-main.519" target="_blank" rel="noopener">Timeline Summarization</a>, <a href="https://aclanthology.org/P19-1210/" target="_blank" rel="noopener">Meeting Summarization</a>, <a href="https://aclanthology.org/N19-4019" target="_blank" rel="noopener">Multilingual</a>, <a href="https://blender.cs.illinois.edu/software/gaia-ie/" target="_blank" rel="noopener">GAIA IE</a></li>
           
           <li>AI for Science <a href="https://aclanthology.org/2022.acl-demo.13" target="_blank" rel="noopener">COVID-19 Claim Radar</a>, <a href="https://aclanthology.org/2021.naacl-demos.8.pdf" target="_blank" rel="noopener">COVID Knowledge Graph</a></li>
           
         </ul>
       </div>
       
 
       
       
 
       
 
     </div>
   </div>
 </div>

  

  </div>
</section>


  

























































<section id="news" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="row">
    <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">News</h1>
            
    </div>
    <div class="col-12 col-lg-8">
        <ul>
<li>[2024-12] Will organize the tutorial on Foundation Models for Embodied Agents at AAAI 2025 and NAACL 2025, with Yunzhu Li, Jiayuan Mao and Wenlong Huang.</li>
<li>[2024-12] Selected as AAAI 2025 New Faculty Highlights.</li>
<li>[2024-12] We released <a href="https://hourvideo.stanford.edu" target="_blank" rel="noopener">HourVideo</a>, consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks.</li>
<li>[2024-11] Organized EMNLP 2024 Bird of Feather on <a href="https://x.com/ManlingLi_/status/1856342201044738112" target="_blank" rel="noopener">LLMs for Embodied Agents</a> with Joyce Chai, Parisa Kordjamshidi, Shuyan Zhou, Xudong Lin.</li>
<li>[2024-11] Our Embodied Agent Interface has been selected as Best Paper Award (top 0.4%) at <a href="https://socalnlp.github.io/symp24/index.html" target="_blank" rel="noopener">SoCal NLP 2024</a>!</li>
<li>[2024-10] Our Embodied Agent Interface has been selected as Oral Presentation (top 0.6% over D&amp;B track, top 0.4% over all tracks) at NeurIPS 2024!</li>
<li>[2024-10] We released <a href="https://embodied-agent-interface.github.io" target="_blank" rel="noopener">Embodied Agent Interface</a> with 100+ page analysis and built up symbolic simulators on BEHAVIOR.</li>
<li>[2024-08] ACL 2024 Outstanding Paper on <a href="https://arxiv.org/abs/2305.12798" target="_blank" rel="noopener">LM-Steer</a>. A big shoutout to Chi Han, a star student awarded NAACL 2024 Outstanding Paper and ACL 2024 Outstanding Paper, and will be on the faculty job market!</li>
<li>[2024-08] Serve as NAACL 2025 Organizing Committee (Publication Chairs).</li>
<li>[2024-06] Serve as ACL 2025 Organizing Committee (Virtual Infrastructure Chairs).</li>
<li>[2024-02] Serve as EMNLP 2024 Organizing Committee (Demonstration Chairs). <!-- * [2024-02] Got my very first PhD student [Zihan](https://zihanwang314.github.io/)! Super excited! Thanks for the trust and I will do my best to be the best advisor like Heng. --> <!-- * [2023-12] Keynote Talk "Beyond the Beaten Path: Exploring the Role of Graphs in Multimodal Foundation Models" at [NeurIPS 2023 Workshop on New Frontiers in Graph Learning](https://glfrontiers.github.io/2023/speakers/). Panel discussion with Jiawei Han, Marinka Zitnik, Yanqi Zhou, Mohammad Shoeybi.  --> <!-- * [2023-10] Will organize "Towards Knowledgeable Language Models @ ACL24 Workshop" [Call for Papers](https://knowledgeable-lm.github.io/), submission deadline in May 20, 2024.  --></li>
<li>[2023-08] Joined Stanford Vision and Learning lab as a postdoc working with Prof Jiajun Wu. Excited! <!-- * [2023-07] Served as a session chair for multimodality at ACL 2023. --> <!-- * [2023-06] Will serve as an Area Chair at EMNLP 2023. --></li>
<li>[2023-05] Will join Northwestern Computer Science as a tenure-track assistant professor. <!-- * [2022-12] Will serve as an Area Chair at ACL 2023. --></li>
<li>[2022-10] Will present at EE CS Rising Star workshop in UT Austin on Oct 25-26 as a Riser.</li>
<li>[2022-10] Will present at DARPA Forward event on Oct 4-5 as a DARPA Riser.</li>
<li>[2022-07] Will give a tutorial at NAACL <a href="https://2022.naacl.org/program/tutorials/#t3" target="_blank" rel="noopener">T3: New Frontiers of Information Extraction</a> at 9:00–12:30 on July 10th (Location: Columbia D) with Muhao Chen, Lifu Huang, Ben Zhou, Heng Ji and Dan Roth. Welcome to our tutorial!</li>
</ul>
<!-- * [2022-05] I am thrilled to be selected as a [DARPA Riser](https://www.darpa.mil/news-events/2022-05-12)! Thanks DARPA!  -->
<ul>
<li>[2022-03] Our work on COVID-KG is reported by the department <a href="https://cs.illinois.edu/news/illinois-cs-papers-earn-top-honors-at-conferences" target="_blank" rel="noopener">Illinois CS Papers Earn Top Honors at Conferences</a>.</li>
<li>[2022-03] I am nominated as a <a href="https://www.darpa.mil/news-events/2022-05-12" target="_blank" rel="noopener">DARPA Riser</a> by DARPA.</li>
<li>[2021-10] I am thrilled to receive <a href="https://www.microsoft.com/en-us/research/academic-program/phd-fellowship/#!people" target="_blank" rel="noopener">Microsoft Research PhD Fellowship</a>! Thanks Microsoft!</li>
<li>[2021-08] Did a tutorial at ACL 2021 about <a href="https://cogcomp.seas.upenn.edu/page/tutorial.202108/" target="_blank" rel="noopener">Event-Centric Natural Language Processing</a> with Muhao Chen, Qiang Ning, Hongming Wang, Heng Ji, Kathleen McKeown and Dan Roth.</li>
<li>[2021-06] Our <a href="https://blender.cs.illinois.edu/paper/COVIDKG.pdf" target="_blank" rel="noopener">COVID-KG</a> was awarded as <a href="https://2021.naacl.org/blog/best-demo-award/" target="_blank" rel="noopener">Best Demo Paper at NAACL 2021</a>.</li>
<li>[2021-06] Our work of knowledge extraction and reasoning has been reported by the department <a href="https://cs.illinois.edu/news/manling-li-produces-award-winning-event-schema-research-with-advisor-and-mentor-heng-ji" target="_blank" rel="noopener">Manling Li Produces Award-winning Event Schema Research with Advisor and Mentor Heng Ji</a>.</li>
<li>[2021-05] Awarded <a href="https://cs.illinois.edu/about/awards/graduate-fellowships-awards/cl-and-jane-w-s-liu-award" target="_blank" rel="noopener">C.L. and Jane Liu Award</a> for showing exceptional research promise.</li>
<li>[2021-04] Selected as <a href="https://publish.illinois.edu/engr-mavis/" target="_blank" rel="noopener">Mavis Future Faculty Fellow</a>.</li>
<li>[2021-02] Did a tutorial at AAAI 2021 about <a href="https://cogcomp.seas.upenn.edu/page/tutorial.202102/" target="_blank" rel="noopener">Event-centric Natural Language Understanding</a> with Muhao Chen, Qiang Ning, Hongming Wang, Heng Ji and Dan Roth.</li>
<li>[2020-10] Ranked 1st in TAC Streaming Multimedia Knowledge Base Population (SM-KBP) 2020.</li>
<li>[2020-08] Our <a href="https://blender.cs.illinois.edu/software/gaia-ie/" target="_blank" rel="noopener">GAIA</a> multimedia IE system has been awarded as <a href="https://acl2020.org/blog/ACL-2020-best-papers" target="_blank" rel="noopener">Best Demo Paper at ACL 2020</a>.</li>
<li>[2019-06] Invited to present our <a href="">event-tracking demo</a> at DARPA Demo Day and ARL Transition Day.</li>
<li>&hellip;</li>
</ul>

    </div>
  </div>


  

  </div>
</section>


  

























































<section id="publications" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="row">
    <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Publications</h1>
            
    </div>
    <div class="col-12 col-lg-8">
        <div class="alert alert-note">
  <div>
    Please find my latest publications on <a href="https://scholar.google.com/citations?user=6U4SXnUAAAAJ&hl=en">Google Scholar</a>.
  </div>
</div>
<br>
<!-- ### Books -->
<!-- <br> -->
<!-- **Advances in Information Access** <br> -->
<!-- Sha Li, <ins>**Manling Li**</ins>, Heng Ji<br>  -->
<!-- **Ongoing (World Scientifc Publishing)**  <br> -->
<h3 id="-denotes-equal-contribution-supsup-denotes-mentored-students">* denotes equal contribution, <sup>†</sup> denotes mentored students</h3>
<br>
<h4 id="2025">2025</h4>
<br>
<p><strong>RAGEN: A General-Purpose Reasoning Agent Training Framework</strong> [<a href='https://embodied-agent-interface.github.io/'>Website</a>][<a href='http://github.com/ZihanWang314/ragen'>Code</a>]    <br>
Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, <ins><strong>Manling Li</strong></ins>    <br>
<span style="color: grey; font-size: 0.9em"><em>The first reproduction of DeepSeek-R1 framework for agent training, featuring:</em> <br>
<em>- No SFT, just direct rule-based RL</em> <br>
<em>- Step-wise rollout-update</em> <br>
<em>- Unified multi-round processing with stable batch sizes</em> <br>
<em>- Potential world modeling capabilities</em> <br> </span></p>
<p><strong>Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models</strong> [<a href='https://arxiv.org/abs/2403.17359'>PDF</a>] <br>
Zhenyu Pan, Haozheng Luo, <ins><strong>Manling Li</strong></ins>, Han Liu<br>
<strong>ICLR 2025</strong>  <br></p>
<p><strong>Foundation Models Meet Embodied Agents</strong> [<a href=''>Website</a>] <br>
<ins><strong>Manling Li</strong></ins>, Yunzhu Li, Jiayuan Mao, Wenlong Huang<br>
<strong>AAAI 2025: Tutorial</strong>  <br>
<strong>NAACL 2025: Tutorial</strong>  <br></p>
<p><strong>LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models</strong> [<a href='https://ai.stanford.edu/~sunfanyun/layoutvlm/'>Website</a>][<a href='https://arxiv.org/abs/2412.02193'>PDF</a>][<a href='https://github.com/sunfanyunn/LayoutVLM'>Code</a>] <br>
Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, <ins><strong>Manling Li</strong></ins>, Nick Haber, Jiajun Wu <br>
<strong>arXiv</strong>  <br></p>
<br>
<h4 id="2024">2024</h4>
<br>
<p><strong>Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</strong> [<a href='https://embodied-agent-interface.github.io/'>Website</a>][<a href='https://arxiv.org/abs/2410.07166'>PDF</a>][<a href='https://github.com/embodied-agent-eval/embodied-agent-eval'>Code</a>][<a href='https://huggingface.co/datasets/Inevitablevalor/EmbodiedAgentInterface'>Data</a>][<a href='https://hub.docker.com/r/jameskrw/eai-eval'>Docker</a>][<a href='https://pypi.org/project/eai-eval/'>PyPi</a>][<a href='https://embodied-agent-eval.readthedocs.io/en/latest/#'>Doc</a>]   <br>
<ins><strong>Manling Li</strong></ins><sup>*</sup>, Shiyu Zhao<sup>*</sup>, Qineng Wang<sup>*</sup>, Kangrui Wang<sup>*</sup>, Yu Zhou<sup>*</sup>, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu <br>
<strong>NeurIPS 2024 Benchmark Track</strong>  (<span style="color:#8C0000"><strong>Oral, Top 0.6%</strong></span>) <br>
<span style="color:#8C0000"><strong>Best Paper Award at <a href="https://socalnlp.github.io/symp24/index.html" target="_blank" rel="noopener">SoCal NLP 2024</a>, Top 0.4%</strong></span><br></p>
<p><strong>HourVideo: 1-Hour Video-Language Understanding</strong> [<a href='https://hourvideo.stanford.edu'>Website</a>][<a href='https://arxiv.org/abs/2411.04998'>PDF</a>][<a href='https://huggingface.co/datasets/HourVideo/HourVideo'>Data</a>][<a href='https://github.com/keshik6/HourVideo'>Code</a>]<br>
Keshigeyan Chandrasegaran, Agrim Gupta, Taran Kota, Lea M. Hadzic, Jimming He, Cristobal Eyzaguirre, Zane Durante, <ins><strong>Manling Li</strong></ins>, Jiajun Wu, Li Fei-Fei <br>
<strong>NeurIPS 2024 Benchmark Track</strong>  <br></p>
<p><strong>IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos</strong> [<a href='https://yunongliu1.github.io/ikea-video-manual/'>Website</a>][<a href='https://arxiv.org/pdf/2411.11409'>PDF</a>][<a href='https://github.com/yunongLiu1/IKEA-Manuals-at-Work'>Data</a>][<a href='https://github.com/yunongLiu1/IKEA-Manuals-at-Work'>Code</a>] <br>
Yunong Liu, Weiyu Liu, Shubh Khanna, Cristobal Eyzaguirre, <ins><strong>Manling Li</strong></ins>, Juan Carlos Niebles, Vineeth Ravi, Saumitra Mishra, Jiajun Wu <br>
<strong>NeurIPS 2024 Benchmark Track</strong>  <br></p>
<p><strong>Visually Descriptive Language Modeling for Vector Graphics Reasoning</strong> [<a href='https://arxiv.org/abs/2404.06479'>PDF</a>][<a href='https://mikewangwzhl.github.io/VDLM'>Website</a>][<a href='https://github.com/MikeWangWZHL/VDLM'>Code</a>]  <br>
Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, <ins><strong>Manling Li</strong></ins>, Jiajun Wu, Heng Ji<br>
<strong>arXiv</strong>  <br></p>
<p><strong>LM-Steer: Word Embeddings Are Steers for Language Models</strong> [<a href='https://arxiv.org/abs/2305.12798'>PDF</a>] <br>
Chi Han, Jialiang Xu, <ins><strong>Manling Li</strong></ins>, Yi Fung, Chenkai Sun, Nan Jiang, Tarek Abdelzaher, Heng Ji<br>
<strong>ACL 2024</strong> <br>
(<span style="color:#8C0000"><strong>Outstanding Paper Award at ACL 2024</strong></span>)<br></p>
<p><strong>Why Does New Knowledge Create Messy Ripple Effects in LLMs?</strong> [<a href='https://arxiv.org/abs/2407.12828'>PDF</a>] <br>
Jiaxin Qin, Zixuan Zhang, Chi Han, Pengfei Yu, <ins><strong>Manling Li</strong></ins>, Heng Ji<br>
<strong>EMNLP 2024</strong>  <br></p>
<p><strong>Deep Concept Injection for Zero-shot Multimodal Reasoning</strong> [<a href=''>PDF</a>] <br>
Xudong Lin, <ins><strong>Manling Li</strong></ins>, Richard Zemel, Heng Ji, Shih-Fu Chang<br>
<strong>EMNLP 2024</strong>  <br></p>
<p><strong>Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models</strong> [<a href='https://arxiv.org/abs/2407.08039'>PDF</a>] <br>
Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi Fung, Jing Li, <ins><strong>Manling Li</strong></ins>, Heng Ji<br>
<strong>arXiv</strong>  <br></p>
<p><strong>MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders</strong> [<a href='https://arxiv.org/pdf/2410.06845'>PDF</a>] <br>
Cheng Li, May Fung, Qingyun Wang, Chi Han, <ins><strong>Manling Li</strong></ins>, Jindong Wang, Heng Ji <br>
<strong>arXiv</strong>  <br></p>
<p><strong>Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate</strong> [<a href='https://arxiv.org/abs/2402.07401.pdf'>PDF</a>] <br>
Kyungha Kim*, Sangyun Lee*, Kung-Hsiang Huang*, Hou Pong Chan, <ins><strong>Manling Li</strong></ins>, Heng Ji<br>
<strong>arXiv</strong>  <br></p>
<p><strong>InfoPattern: Unveiling Information Propagation Patterns in Social Media</strong> [<a href='https://arxiv.org/abs/2311.15642.pdf'>PDF</a>] <br>
Chi Han*, Jialiang Xu*, <ins><strong>Manling Li</strong></ins>* , Hanning Zhang*, Tarek Abdelzaher, Heng Ji<br>
<strong>arXiv</strong>  <br></p>
<p><strong>SmartBook: AI-Assisted Situation Report Generation</strong> [<a href='https://arxiv.org/pdf/2303.14337.pdf'>PDF</a>] <br>
Revanth Gangi Reddy, Yi Fung, Qi Zeng, <ins><strong>Manling Li</strong></ins>, Zihan Wang, Paul Sullivan, Heng Ji<br>
<strong>arXiv</strong>  <br></p>
<p><strong>Controlling Object Existence Hallucinations in Large Vision Language Models</strong> [<a href='https://arxiv.org/pdf/2310.01779.pdf'>PDF</a>] <br>
Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Chunyuan Li, <ins><strong>Manling Li</strong></ins><br>
<strong>arXiv</strong>  <br></p>
<br>
<h4 id="2023">2023</h4>
<br>
<p><strong>Event-centric Multimodal Knowledge Acquisition</strong> [<a href='uploads/ManlingLiThesis.pdf'>PDF</a>] <br>
<ins><strong>Manling Li</strong></ins><br>
Thesis Committee: Heng Ji, Jiawei Han, Chengxiang Zhai, Shih-Fu Chang, Kyunghyun Cho<br>
<strong>Thesis</strong>  <br></p>
<p><strong>ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation</strong> [<a href=''>PDF</a>] <br>
Yangyi Chen, Xingyao Wang, <ins><strong>Manling Li</strong></ins>, Derek Hoiem, Heng Ji<br>
<strong>EMNLP 2023</strong>  <br></p>
<p><strong>Defining a New NLP Playground</strong> [<a href=''>PDF</a>] <br>
Sha Li, Chi Han, Pengfei Yu, Carl Edwards, <ins><strong>Manling Li</strong></ins>, Xingyao Wang, Yi Fung, Charles Yu, Joel R. Tetreault, Eduard H Hovy, Heng Ji<br>
<strong>EMNLP 2023 Findings</strong>  <br></p>
<p><strong>Knowledge-Driven Vision-Language Encoding</strong> [<a href='https://blender.cs.illinois.edu/tutorial/KnowledgeVLP/'>Website</a>] <br>
<ins><strong>Manling Li</strong></ins>, Xudong Lin, Jie Lei, Mohit Bansal, Carl Vondrick, Shih-Fu Chang, Heng Ji<br>
<strong>CVPR 2023: Tutorial</strong>  <br></p>
<p><strong>Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval</strong> [<a href='https://openreview.net/forum?id=C9hEF7lYOSP'>PDF</a>] [<a href=''>Code</a>] <br>
Xudong Lin, Simran Tiwari, Shiyuan Huang, <ins><strong>Manling Li</strong></ins>, Mike Zheng Shou, Heng Ji, Shih-Fu Chang<br>
<strong>CVPR 2023</strong> <br></p>
<p><strong>Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting</strong> [<a href='https://blender.cs.illinois.edu/tutorial/KnowledgeVLP/'>Website</a>] <br>
Hejie Cui, Xinyu Fang, Zihan Zhang, Ran Xu, Xuan Kan, Xin Liu, <ins><strong>Manling Li</strong></ins>, Yangqiu Song, Carl Yang<br>
<strong>NeurIPS 2023</strong>  <br></p>
<p><strong>Non-Sequential Graph Script Induction via Multimedia Grounding</strong> [<a href=''>PDF</a>] <br>
Yu Zhou<sup>†</sup>, Sha Li, <ins><strong>Manling Li</strong></ins>, Xudong Lin, Shih-Fu Chang, Mohit Bansal and Heng Ji<br>
<strong>ACL 2023</strong> (<sup>†</sup> denotes supervised undergraduate) <br></p>
<p><strong>A Language First Approach to Procedure Planning</strong> [<a href=''>PDF</a>] <br>
Jiateng Liu<sup>†</sup>, Sha Li, Zhenhailong Wang, <ins><strong>Manling Li</strong></ins>, Heng Ji<br>
<strong>ACL 2023 Findings</strong> (<sup>†</sup> denotes supervised undergraduate) <br></p>
<p><strong>Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification</strong> [<a href=''>PDF</a>] <br>
Sha Li, Ruining Zhao<sup>†</sup>, <ins><strong>Manling Li</strong></ins>, Heng Ji, Chris Callison-Burch and Jiawei Han<br>
<strong>ACL 2023</strong> (<sup>†</sup> denotes supervised undergraduate) <br></p>
<p><strong>Multimedia Generative Script Learning for Task Planning</strong> [<a href=''>PDF</a>] <br>
Qingyun Wang, <ins><strong>Manling Li</strong></ins>, Hou Pong Chan, Lifu Huang, Julia Hockenmaier, Girish Chowdhary and Heng Ji<br>
<strong>ACL 2023 Findings</strong>  <br></p>
<p><strong>Learning to Decompose Visual Features with Latent Textual Prompts</strong> [<a href='https://arxiv.org/abs/2210.04287'>PDF</a>] [<a href=''>Code</a>] <br>
Feng Wang<sup>†</sup>, <ins><strong>Manling Li</strong></ins>, Xudong Lin, Hairong Lv, Alexander Schwing, Heng Ji<br>
<strong>ICLR 2023</strong> (<sup>†</sup> denotes supervised undergraduate) <br></p>
<p><strong>Knowledge-Driven Vision-Language Pretraining</strong> [<a href=''>PDF</a>] [<a href='https://blender.cs.illinois.edu/tutorial/knowledgeVLP/'>Website</a>] <br>
<ins><strong>Manling Li</strong></ins>, Xudong Lin, Jie Lei, Mohit Bansal, Shih-Fu Chang, Heng Ji<br>
<strong>AAAI 2023: Tutorial</strong>  <br></p>
<p><strong>Video Event Extraction via Tracking Visual States of Arguments</strong> [<a href='https://arxiv.org/abs/2211.01781'>PDF</a>] [<a href='https://github.com/Shinetism/VidSitu-EC'>Code</a>] <br>
Guang Yang<sup>†</sup>, <ins><strong>Manling Li</strong></ins>, Jiajie Zhang, Xudong Lin, Shih-Fu Chang, Heng Ji<br>
<strong>AAAI 2023</strong> (<sup>†</sup> denotes supervised undergraduate) <br></p>
<p><strong>ADEPT: A DEbiasing PrompT Framework</strong> [<a href='https://arxiv.org/abs/2211.05414'>PDF</a>] [<a href='https://github.com/EmpathYang/ADEPT'>Code</a>] <br>
Ke Yang<sup>†</sup>, Charles Yu, Yi Fung, <ins><strong>Manling Li</strong></ins>, Heng Ji<br>
<strong>AAAI 2023</strong> (<sup>†</sup> denotes supervised undergraduate) <br></p>
<br>
<h4 id="2022">2022</h4>
<br>
<p><strong>Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners</strong> [<a href='https://arxiv.org/abs/2205.10747'>PDF</a>] [<a href='https://github.com/MikeWangWZHL/VidIL'>Code</a>] <br>
Zhenhailong Wang<sup>†</sup>*,<ins><strong>Manling Li</strong></ins>*, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji<br>
<strong>NeurIPS'22</strong> (equal contribution) <br></p>
<p><strong>CLIP-Event:Connecting Vision and Text with Event Structures</strong> [<a href='https://arxiv.org/abs/2201.05078'>PDF</a>] [<a href='https://github.com/limanling/clip-event'>Data</a>] [<a href='https://github.com/limanling/clip-event'>Code</a>] <br>
<ins><strong>Manling Li</strong></ins>, Ruochen Xu, Shuohang Wang, Xudong Lin, Chenguang Zhu, Xuedong Huang, Heng Ji, Shih-Fu Chang<br>
<strong>CVPR'22</strong> (<span style="color:#8C0000"><strong>Oral, Top 4.1%</strong></span>) <br></p>
<p><strong>COVID-19 Claim Radar: A Structured Claim Extraction and Tracking System</strong> [<a href=''>PDF</a>] [<a href='https://github.com/blender-nlp/covid-claim-radar'>Code</a>] [<a href='http://18.221.187.153/'>Demo</a>] [<a href='http://blender.cs.illinois.edu/aida/covid_claim_radar.mp4'>Video</a>] <br>
<ins><strong>Manling Li</strong></ins>, Revanth Gangi Reddy, Ziqi Wang, Yi-Shyuan Chiang, Tuan M. Lai, Pengfei Yu, Zixuan Zhang,Heng Ji<br>
<strong>ACL'22 Demo</strong>  <br></p>
<p><strong>Event Schema Induction with Double Graph Autoencoders</strong> [<a href='https://aclanthology.org/2022.naacl-main.147/'>PDF</a>] [<a href='https://github.com/tracyjin/DoubleGAE'>Code</a>] <br>
Xiaomeng Jin<sup>†</sup>, <ins><strong>Manling Li</strong></ins> and Heng Ji<br>
<strong>NAACL'22</strong>  <br></p>
<p><strong>New Frontiers of Information Extraction</strong> [<a href='https://aclanthology.org/2022.naacl-tutorials.3/'>PDF</a>] [<a href='https://cogcomp.seas.upenn.edu/page/tutorial.202207/'>Website</a>] [<a href='https://cogcomp.seas.upenn.edu/page/tutorial.202207/'>Slides</a>] [<a href='https://underline.io/events/325/sessions/11172/lecture/55589-t3-new-frontiers-of-information-extraction'>Videos</a>]<br>
Muhao Chen, Lifu Huang, <ins><strong>Manling Li</strong></ins>, Ben Zhou, Heng Ji<br>
<strong>NAACL'22: Tutorial</strong>  <br></p>
<p><strong>MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding</strong> [<a href='https://arxiv.org/abs/2112.10728'>PDF</a>] [<a href='https://github.com/blender-nlp/MuMuQA'>Data</a>] <br>
Revanth Gangi Reddy<sup>†</sup>, Xilin Rui<sup>†</sup>, <ins><strong>Manling Li</strong></ins>, Xudong Lin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mohit Bansal, Avi Sil, Shih-Fu Chang, Alexander Schwing, Heng Ji<br>
<strong>AAAI'22</strong>  <br></p>
<br>
<h4 id="2021">2021</h4>
<br>
<p><strong>The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction</strong> [<a href='https://blender.cs.illinois.edu/paper/schema2021a.pdf'>PDF</a>] [<a href='https://github.com/limanling/temporal-graph-schema'>Data</a>] <br>
<ins><strong>Manling Li</strong></ins>, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng Ji, Jiawei Han and Clare Voss<br>
<strong>EMNLP'21</strong>  <br></p>
<p><strong>Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport</strong> [<a href=''>PDF</a>] [<a href=''>Data</a>] <br>
<ins><strong>Manling Li</strong></ins>, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao, Heng Ji and Kathleen McKeown<br>
<strong>EMNLP'21</strong>  <br></p>
<p><strong>Joint Multimedia Event Extraction from Video and Article</strong> [<a href=''>PDF</a>] [<a href=''>Data</a>] <br>
Brian Chen, Xudong Lin, Christopher Thomas, <ins><strong>Manling Li</strong></ins>, Shoya Yoshida, Lovish Chum, Heng Ji and Shih-Fu Chang<br>
<strong>EMNLP'21</strong> Findings  <br></p>
<p><strong>Event-centric Natural Language Processing</strong> [<a href='https://blender.cs.illinois.edu/paper/eventtutorial2021.pdf'>PDF</a>] [<a href='https://blender.cs.illinois.edu/paper/aaai_tutorial_2021_event_centric_nlu.pdf'>Slides</a>] <br>
Muhao Chen, Hongming Zhang, Qiang Ning, <ins><strong>Manling Li</strong></ins>, Heng Ji, Kathleen McKeown and Dan Roth<br>
<strong>ACL'21</strong>: Tutorial.  <br></p>
<p><strong>COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation</strong> [<a href='https://blender.cs.illinois.edu/paper/COVIDKG.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/covid19/'>Code/Data</a>]<br>
Qingyun Wang, <ins><strong>Manling Li</strong></ins>, Xuan Wang, Nikolaus Parulian, Guangxing Han, Jiawei Ma, Jingxuan Tu, Ying Lin, Haoran Zhang, Weili Liu, Aabhas Chauhan, Yingjun Guan, Bangzheng Li, Ruisong Li, Xiangchen Song, Heng Ji, Jiawei Han, Shih-Fu Chang, James Pustejovsky, David Liem, Ahmed Elsayed, Martha Palmer, Jasmine Rah, Clare Voss, Cynthia Schneider, Boyan Onyshkevych <br>
<strong>NAACL'21</strong>: System Demonstrations <br>
(<span style="color:#8C0000"><strong>Best Demo Paper Award at NAACL2021</strong></span>)<br></p>
<p><strong>RESIN: A Dockerlized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</strong> [<a href='https://blender.cs.illinois.edu/paper/resin-phase1.pdf'>PDF</a>] [<a href='https://github.com/RESIN-KAIROS/RESIN-pipeline-public'>Code</a>] <br>
Haoyang Wen, Ying Lin, Tuan M. Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, <ins><strong>Manling Li</strong></ins>, Haoyu Wang, Hongming Zhang, Xiaodong Yu, Alexander Dong, Zhenhailong Wang, Yi R. Fung, Piyush Mishra, Qing Lyu, Dídac Surís, Brian Chen, Susan W. Brown, Martha Palmer, Chris Callison-Burch, Carl Vondrick, Jiawei Han, Dan Roth, Shih-Fu Chang and Heng Ji<br>
<strong>NAACL'21</strong>: System Demonstrations <br></p>
<p><strong>Event-centric Natural Language Processing</strong> [<a href='https://blender.cs.illinois.edu/paper/eventtutorial2021.pdf'>PDF</a>] [<a href='https://blender.cs.illinois.edu/paper/aaai_tutorial_2021_event_centric_nlu.pdf'>Slides</a>] <br>
Muhao Chen, Hongming Zhang, Qiang Ning, <ins><strong>Manling Li</strong></ins>, Heng Ji and Dan Roth<br>
<strong>AAAI'21</strong>: Tutorial.  <br></p>
<br>
<h4 id="2020">2020</h4>
<br>
<p><strong>Connecting the Dots: Event Graph Schema Induction with Path Language Modeling</strong> [<a href='https://blender.cs.illinois.edu/paper/eventgraphschema2020.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/software/pathlm'>Code/Data</a>] [<a href='docs/paper237-schema-presentation.pdf'>Slides</a>] <br>
<ins><strong>Manling Li</strong></ins>, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers and Clare Voss <br>
<strong>EMNLP'20</strong>: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.  <br></p>
<p><strong>GAIA: A Fine-grained Multimedia Knowledge Extraction System</strong> [<a href='https://blender.cs.illinois.edu/paper/aidaacl2020demo.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/software/gaia-ie'>Code</a>] [<a href='http://blender.cs.illinois.edu/software/gaia-ie/gaia.mp4'>Video</a>]<br>
<ins><strong>Manling Li</strong></ins>*, Alireza Zareian*, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare R. Voss,  Dan Napierski, Marjorie Freedman <br>
<strong>ACL'20</strong>: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 77–86 <br>
(<span style="color:#8C0000"><strong>Best Demo Paper Award at ACL2020</strong></span>) <br></p>
<p><strong>Cross-media Structured Common Space for Multimedia Event Extraction</strong> [<a href='https://blender.cs.illinois.edu/paper/multimediaspace2020.pdf'>PDF</a>] [<a href='http://blender.cs.illinois.edu/software/m2e2'>Code</a>] [<a href='docs/ACL20-m2e2_presentation.pdf'>Slides</a>]<br>
<ins><strong>Manling Li</strong></ins>*, Alireza Zareian*, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang <br>
<strong>ACL'20</strong>: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.2557–2568 <br></p>
<p><strong>GAIA at SM-KBP 2020 - A Dockerized Multi-media Multi-lingual Knowledge Extraction, Clustering, Temporal Tracking and Hypothesis Generation System</strong> [<a href='https://dsr.cise.ufl.edu/wp-content/uploads/2021/02/gaia_smkbp_2020.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2020/KBP/SM-KBP/index.html'>Project</a>] <br>
<ins><strong>Manling Li</strong></ins>, Ying Lin, Tuan Manh Lai, Xiaoman Pan, Haoyang Wen, Sha Li, etc %Zhenhailong Wang, Pengfei Yu, Lifu Huang, Di Lu, Qingyun Wang, Haoran Zhang, Qi Zeng, Chi Han, Zixuan Zhang, Yujia Qin, Xiaodan Hu, Nikolaus Parulian, Daniel Campos, Heng Ji, Brian Chen, Xudong Lin, Alireza Zareian, Amith Ananthram, Emily Allaway, Shih-Fu Chang, Kathleen McKeown, Yixiang Yao, Yifan Wang, Michael Spector, Mitchell DeHaven, Daniel Napierski, Marjorie Freedman, Pedro Szekely, Haidong Zhu, Ram Nevatia, Yang Bai, Yifan Wang, Ali Sadeghian, Haodi Ma, Daisy Zhe Wang <br>
<strong>TAC-KBP</strong>: Text Analysis Conference Knowledge Base Population Workshop 2020 (<span><strong>Rank 1st in the leaderboard.</strong></span>)<br></p>
<!-- **UIUC TAC2020 RUFES System Description**  [<a href='https://blender.cs.illinois.edu/paper/rufesuiuc2020.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2020/KBP/RUFES/index.html'>Project</a>] 
<br>
Revanth Gangi Reddy, <ins>**Manling Li**</ins>, Haoyang Wen and Heng Ji<br>
**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2020 <br> -->
<br>
<h4 id="2019">2019</h4>
<br>
<p><strong>Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization</strong><br>
[<a href='docs/multimediasummarization2019.pdf'>PDF</a>] [<a href='https://github.com/limanling/MeetingSum'>Code</a>]
<br>
<ins><strong>Manling Li</strong></ins>, Lingyu Zhang, Heng Ji, Rich Radke <br>
<strong>ACL'19</strong>: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.2190–2196 <br></p>
<p><strong>Multilingual Entity, Relation, Event and Human Value Extraction</strong>  [<a href='https://blender.cs.illinois.edu/paper/naacldemo2019.pdf'>PDF</a>] [<a href='https://github.com/limanling/uiuc_ie_pipeline_coarse_grained'>Code</a>] [<a href='https://youtu.be/cQPHaxGLn8k'>Video</a>] <br>
<ins><strong>Manling Li</strong></ins>, Ying Lin, Joe Hoover, Spencer Whitehead, Clare Voss, Morteza Dehghani, Heng Ji <br>
<strong>NAACL'19</strong>: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp.110–115 <br></p>
<!-- **The Unobtrusive Group Interaction (UGI) Corpus** [<a href='docs/UGI.pdf'>PDF</a>] [<a href='https://sites.google.com/view/ugirpi'>Code</a>]  <br>
Indrani Bhattacharya, Michael Foley, Ni Zhang, Tongtao Zhang, Christine Ku, Cameron Mine, <ins>**Manling Li**</ins>, Heng Ji, etc. <br>
**ACM MMSys'19**: ACM Multimedia Systems Conference 2019, pp.249-254  <br> -->
<p><strong>GAIA at SM-KBP 2019 - A Multi-media Multi-lingual KnowledgeExtraction and Hypothesis Generation System</strong> [<a href='docs/GAIA2019.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2020/SM-KBP/index.html'>Project</a>] <br>
<ins><strong>Manling Li</strong></ins>, Ying Lin, Ananya Subburathinam, Spencer Whitehead, Xiaoman Pan, Di Lu, Qingyun Wang, Tongtao Zhang, Lifu Huang, Heng Ji, Alireza Zareian, Hassan Akbari, Brian Chen, Bo Wu, Emily Allaway,
Shih-Fu Chang, Kathleen McKeown, Yixiang Yao, Jennifer Chen, Eric Berquist, Kexuan Sun, Xujun Peng, Ryan Gabbard
Marjorie Freedman, Pedro Szekely, T.K. Satish Kumar, Arka Sadhu, Ram Nevatia, Miguel Rodriguez, Yifan Wang, Yang Bai, Ali Sadeghian, Daisy Zhe Wang <br>
<strong>TAC-KBP</strong>: Text Analysis Conference Knowledge Base Population Workshop 2019 (<span><strong>Rank 1st, with more than 10% higher than the second team.</strong></span>)<br></p>
<!-- **A Baseline Fine-Grained Entity Extraction System for TAC-KBP2019**  [<a href='docs/UIUC_TAC_KBP2019_Fine_Grained_Entity_Extraction_System.pdf'>PDF</a>] [<a href='https://tac.nist.gov/2019/workshop/tac2019.general.html'>Project</a>] 
<br>
Ying Lin, Xiaoman Pan, <ins>**Manling Li**</ins>, Heng Ji<br>
**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2019 <br> -->
<br>
<h4 id="2018-and-before">2018 and Before</h4>
<br>
<p>Please see my full list at <a href="https://scholar.google.com/citations?user=6U4SXnUAAAAJ&hl=en">Google Scholar</a>.</p>
<!-- 2. **GAIA - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System**  [<a href='docs/GAIA.pdf'>PDF</a>] <br>
Tongtao Zhang, Ananya Subburathinam, Ge Shi, Lifu Huang, Di Lu, Xiaoman Pan, <ins>**Manling Li**</ins>, Boliang Zhang, Qingyun Wang, Spencer Whitehead, Heng Ji, etc. <br>
**TAC-KBP**: Text Analysis Conference Knowledge Base Population Workshop 2018  <br>  --> 
<!-- **Link Prediction in Knowledge Graphs: A Hierarchy-Constrained Approach**  [<a href='https://ieeexplore.ieee.org/document/8450054'>PDF</a>] [<a href=''>Code</a>] <br>
<ins>**Manling Li**</ins>, Yuanzhuo Wang, Denghui Zhang, Yantao Jia, Xueqi Cheng <br>
IEEE Transactions on Big Data, pp.1-14 <br>
Special Issue on "Knowledge Graphs: Techniques and Applications"   -->
<!-- **Hierarchical Types Constrained Topic Entity Detection for Knowledge Base Question Answering**  [<a href='https://dl.acm.org/doi/abs/10.1145/3184558.3186916'>PDF</a>] <br>
Yunqi Qiu, <ins>**Manling Li**</ins>, Yuanzhuo Wang, Yantao Jia, Xiaolong Jin, Xueqi Cheng <br>
**WWW 2018**: Companion Proceedings of The Web Conference 2018, pp.35-36  (abstract paper) <br> -->
<!-- 
**Path-Based Attention Neural Model for Fine-Grained Entity Typing**  [<a href='docs/PAN.pdf'>PDF</a>] [<a href='https://github.com/zdh2292390/PAN'>Code</a>]<br>
Denghui Zhang, <ins>**Manling Li**</ins>, Pengshan Cai, Yantao Jia,  Yuanzhuo Wang, Xueqi Cheng <br>
**AAAI 2018**: Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp.8179-8180 (abstract paper) <br>
<p><strong>Efficient Parallel Translating Embedding For Knowledge Graphs</strong>  [<a href='docs/ParTransX.pdf'>PDF</a>] [<a href='https://github.com/zdh2292390/ParTrans-X'>Code</a>] <br>
Denghui Zhang, <ins><strong>Manling Li</strong></ins>, Yantao Jia, Yuanzhuo Wang, Xueqi Cheng <br>
<strong>WI 2017</strong>: Proceedings of IEEE/WIC/ACM International Conference on Web Intelligence, pp.460-468<br></p>
<p><strong>OpenKN at TAC KBP 2016</strong> [<a href='docs/TAC2016_ICTCAS_OKN.pdf'>PDF</a>] [<a href='https://tac.nist.gov//2016/KBP/'>Project</a>] <br>
<ins><strong>Manling Li</strong></ins>, Xinlei Chen, Yantao Jia, Yuanzhuo Wang, etc. <br>
<strong>TAC-KBP</strong>: Text Analysis Conference Knowledge Base Population Workshop 2016 <br></p>
<!-- (Cold Start Entity Discovery and Linking: ranked **2nd** out of 7 teams, where in Entity Discovery, ranked **1st** out of 7 teams, and **4** measures ranked **1st** among 6 measures; Cold Start Slot Filling: ranked 9th out of 19 teams)  <br>
--> 
<!-- **Hierarchy-Based Link Prediction in Knowledge Graphs** [<a href='docs/hTransA.pdf'>PDF</a>] [<a href=''>Poster</a>]<br>
<ins>**Manling Li**</ins>, Yantao Jia, Yuanzhuo Wang, Jingyuan Li, Xueqi Cheng. <br> 
**WWW 2016**: Proceedings of the 25th International Conference Companion on World Wide Web, pp.77-78 (abstract paper) <br>


**Predicting Links and Their Building Time: A Path-Based Method** [<a href='docs/TDLP.pdf'>PDF</a>] [<a href=''>Poster</a>] <br>
<ins>**Manling Li**</ins>, Yantao Jia, Yuanzhuo Wang, Zeya Zhao, Xueqi Cheng. <br>
**AAAI 2016**: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp.4228-4229 (abstract paper) <br> -->

    </div>
  </div>


  

  </div>
</section>


  

























































<section id="talks" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="row">
    <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Talks</h1>
            
    </div>
    <div class="col-12 col-lg-8">
        <h3 id="tutorials">Tutorials</h3>
<ul>
<li>
<p><strong>Foundation Models Meet Embodied Agents</strong> <br>
Tutorial at AAAI 2025. Feb, 2025. <br>
Tutorial at NAACL 2025. May, 2024. <br>
With Yunzhu Li, Jiayuan Mao, Wenlong Huang. <br></p>
</li>
<li>
<p><strong>Lifecycle of Knowledge in LLMs: Memorization, Editing, and Beyond</strong> <br>
Tutorial at AAAI 2025. Feb, 2025.<br>
With Yuji Zhang, Zoey Sha Li, Chi Han, Heng Ji. <br></p>
</li>
<li>
<p><strong>Beyond Human Creativity: A Tutorial on Advancements in AI Generated Content (AIGC)</strong> <br>
Tutorial at IJCAI 2024. Aug, 2024. <br>
With Bang Liu, Yu Chen, Heng Ji, Lingfei Wu. <br>
<a href="https://sites.google.com/view/aigc-tutorial/home?authuser=0" target="_blank" rel="noopener">[Website/Slides]</a> <br></p>
</li>
<li>
<p><strong>Large Language Models for NLP in Finance</strong> <br>
Tutorial at ICAIF (ACM International Conference on AI in Finance) 2023. Nov, 2023. <br>
With Simerjot Kaur, Charese Smiley, Xiaomo Liu, Elena Kochkina, Mohammad Ghassemi, Reza Khanmohammadi. <br>
<a href="uploads/slides/LargeLanguageModelsforNLPinFinance.pdf">[Slides]</a> <br></p>
</li>
<li>
<p><strong>Knowledge-Driven Vision-Language Encoding</strong> <br>
Tutorial at CVPR 2023. Jun, 2023. <br>
With <a href="https://xudonglinthu.github.io/" target="_blank" rel="noopener">Xudong Lin</a>, <a href="https://jayleicn.github.io/" target="_blank" rel="noopener">Jie Lei</a>, <a href="https://www.cs.columbia.edu/~vondrick/" target="_blank" rel="noopener">Carl Vondrick</a>, <a href="https://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener">Mohit Bansal</a>, <a href="https://blender.cs.illinois.edu/hengji.html" target="_blank" rel="noopener">Heng Ji</a>, and <a href="https://www.ee.columbia.edu/~sfchang/" target="_blank" rel="noopener">Shih-Fu Chang</a>. <br>
<a href="https://blender.cs.illinois.edu/tutorial/KnowledgeVLP/" target="_blank" rel="noopener">[Website/Slides/Videos]</a> <a href="https://github.com/limanling/KnowledgeVL-Reading" target="_blank" rel="noopener">[ReadingList]</a><br></p>
</li>
<li>
<p><strong>Knowledge-Driven Vision-Language Pretraining</strong> <br>
Tutorial at AAAI 2023. Feb, 2023. <br>
With <a href="https://xudonglinthu.github.io/" target="_blank" rel="noopener">Xudong Lin</a>, <a href="https://jayleicn.github.io/" target="_blank" rel="noopener">Jie Lei</a>, <a href="https://www.cs.unc.edu/~mbansal/" target="_blank" rel="noopener">Mohit Bansal</a>, <a href="https://blender.cs.illinois.edu/hengji.html" target="_blank" rel="noopener">Heng Ji</a>, and <a href="https://www.ee.columbia.edu/~sfchang/" target="_blank" rel="noopener">Shih-Fu Chang</a>. <br>
<a href="https://blender.cs.illinois.edu/tutorial/knowledgeVLP/" target="_blank" rel="noopener">[Website/Slides/Videos]</a> <a href="https://github.com/limanling/KnowledgeVL-Reading" target="_blank" rel="noopener">[ReadingList]</a><br></p>
</li>
<li>
<p><strong>New Frontiers of Information Extraction</strong> <br>
Tutorial at NAACL 2022. Jul, 2022. <br>
With <a href="https://muhaochen.github.io/" target="_blank" rel="noopener">Muhao Chen</a>, <a href="https://wilburone.github.io/" target="_blank" rel="noopener">Lifu Huang</a>, <a href="http://xuanyu.me/" target="_blank" rel="noopener">Ben Zhou</a>, <a href="https://blender.cs.illinois.edu/hengji.html" target="_blank" rel="noopener">Heng Ji</a>, and <a href="https://www.cis.upenn.edu/~danroth/" target="_blank" rel="noopener">Dan Roth</a>. <br>
<a href="https://cogcomp.seas.upenn.edu/page/tutorial.202207/" target="_blank" rel="noopener">[Website/Slides/Videos]</a> <a href="https://aclanthology.org/2022.naacl-tutorials.3/" target="_blank" rel="noopener">[PDF]</a></p>
</li>
<li>
<p><strong>Event-centric Natural Language Understanding</strong> <br>
Tutorial at ACL2021. Aug 2021. <br>
With <a href="https://muhaochen.github.io/" target="_blank" rel="noopener">Muhao Chen</a>, <a href="https://www.qiangning.info/" target="_blank" rel="noopener">Qiang Ning</a>, <a href="https://www.cse.ust.hk/~hzhangal/" target="_blank" rel="noopener">Hongming Zhang</a>, <a href="http://blender.cs.illinois.edu/hengji.html" target="_blank" rel="noopener">Heng Ji</a>, <a href="http://www.cs.columbia.edu/~kathy/" target="_blank" rel="noopener">Kathleen McKeown</a>, and <a href="https://www.cis.upenn.edu/~danroth/" target="_blank" rel="noopener">Dan Roth</a>. <br>
<a href="https://cogcomp.seas.upenn.edu/page/tutorial.202108/" target="_blank" rel="noopener">[Website/Slides/Videos]</a> <a href="https://aclanthology.org/2021.acl-tutorials.2/" target="_blank" rel="noopener">[PDF]</a></p>
</li>
<li>
<p><strong>Event-centric Natural Language Processing</strong> <br>
Tutorial at AAAI2021. Feb, 2021. <br>
With <a href="https://muhaochen.github.io/" target="_blank" rel="noopener">Muhao Chen</a>, <a href="https://www.qiangning.info/" target="_blank" rel="noopener">Qiang Ning</a>, <a href="https://www.cse.ust.hk/~hzhangal/" target="_blank" rel="noopener">Hongming Zhang</a>, <a href="http://blender.cs.illinois.edu/hengji.html" target="_blank" rel="noopener">Heng Ji</a>, and <a href="https://www.cis.upenn.edu/~danroth/" target="_blank" rel="noopener">Dan Roth</a>. <br>
<a href="https://cogcomp.seas.upenn.edu/page/tutorial.202102/" target="_blank" rel="noopener">[Website/Slides/Videos]</a> <a href="https://muhaochen.github.io/index_files/AAAI_2021_event.pdf" target="_blank" rel="noopener">[PDF]</a></p>
</li>
</ul>
<h3 id="invited-talks">Invited Talks</h3>
<ul>
<li>
<p><strong>Embodied Agent Interface: LLMs and VLMs for Embodied Reasoning and Planning</strong> <br>
SFU @ NeurIPS 2024. Dec 2024 <br></p>
</li>
<li>
<p><strong>LLMs for Embodied Agents</strong> <br>
EMNLP 2024 Birds of Feather. Nov 2024 <br></p>
</li>
<li>
<p><strong>Customizing Large Language Models to Embodied Agent interacting with Embodied Envioronments</strong> <br>
EMNLP 2024 CustomNLP4U Workshop. Nov 2024 <br></p>
</li>
<li>
<p><strong>From Large Language Models to Large Agent Models</strong> <br>
Keynote at <a href="https://aice.illinois.edu/events" target="_blank" rel="noopener">Amazon-Illinois Center on AI for Interactive Conversational Experiences Fall Research Symposium 2024</a>. Sept 2024 <br></p>
</li>
<li>
<p><strong>Reasoning and Planning with Physical World Knowledge</strong> <br>
<a href="https://allerton.csl.illinois.edu/" target="_blank" rel="noopener">2024 Allerton Conference on Communication, Control, and Computing</a>. Sept 2024 <br></p>
</li>
<li>
<p><strong>Embodied Agent Interface: LLMs for Embodied Decision Making</strong> <br>
<a href="https://sites.google.com/view/multimodal-ai-ttic-2024/home" target="_blank" rel="noopener">TTIC Multimodal AI Workshop 2024</a>. Aug 2024 <br></p>
</li>
<li>
<p><strong>Multimodal Knowledge for Social Good</strong> <br>
<a href="https://sicss.io" target="_blank" rel="noopener">Summer Institute in Computational Social Science 2024</a>. Aug 2024 <br></p>
</li>
<li>
<p><strong>Reasoning, Planning and Compositionality in Multimodality</strong> <br>
<a href="https://splu-robonlp-2024.github.io" target="_blank" rel="noopener">SpLU-RoboNLP 2024</a> Workshop at ACL. Jul 2024 <br></p>
</li>
<li>
<p><strong>Visually Descriptive Language Modeling for Document Intelligence</strong> <br>
Adobe Research. Jul 2024 <br></p>
</li>
<li>
<p><strong>From Large Language Models to Large Agent Models</strong> <br>
<a href="https://machinelearning.apple.com/updates/nlu-workshop-2023" target="_blank" rel="noopener">Apple NLU Workshop 2024</a>. Jun 2024 <br></p>
</li>
<li>
<p><strong>From Words to Worlds: A Close Look to Diffusion Models (through an NLP Lens)</strong> <br>
UIUC NLP Seminar. Jun 2024 <br></p>
</li>
<li>
<p><strong>The Missing Knowledge in LLMs to Interact with the Physical World</strong> <br>
<a href="https://midwest-ml.org/2024/" target="_blank" rel="noopener">Midwest Machine Learning Symposium 2024</a>. May 2024 <br></p>
</li>
<li>
<p><strong>Beyond the Beaten Path: Exploring the Role of Graphs in Multimodal Foundation Models</strong> <br>
Keynote Talk at <a href="https://glfrontiers.github.io/2023/speakers/" target="_blank" rel="noopener">NeurIPS 2023 Workshop on New Frontiers in Graph Learning</a>. Dec 2023 <br></p>
</li>
<li>
<p><strong>LLMs for robotics: Modeling the Knowledge of the Physical World</strong> <br>
Stanford Vision and Learning Seminar. Oct 2023
<br></p>
</li>
<li>
<p><strong>Knowledge Foundation Models</strong> <br>
Adobe Research. Oct 2023 <br>
Tsinghua University. Oct 2023 <br>
Renmin University. Oct 2023 <br>
Fudan University. Oct 2023 <br>
Zhejiang University. Oct 2023 <br>
Peking University. Nov 2023 <br></p>
</li>
<li>
<p><strong>Modeling the Semantics of the Physical World</strong> <br>
Stanford CogAI. Jun 2023
<br></p>
</li>
<li>
<p><strong>Towards Factulity in Information Access: Multimodal Knowledge Acqusition and Reasoning</strong> <br>
Carnegie Mellon University, LTI. Feb 2023 <br>
Northwestern University, CS. Feb 2023 <br>
Northeastern University, ECE. Feb 2023 <br>
Purdue University, CS. Feb 2023 <br>
Rice University, CS. Feb 2023 <br>
Virginia Tech, CS. Feb 2023 <br>
Max Planck Institute. Feb 2023 <br>
UVA, CS. Mar 2023 <br>
MBZUAI. Mar 2023 <br>
U Washington St Louis, CS. Mar 2023 <br>
University of Toronto, CS+ECE. Mar 2023 <br>
UC San Diego, ECE. Feb 2023 <br>
UC Davis, CS. Mar 2023 <br>
UC Los Angeles, ECE. Apr 2023 <br></p>
</li>
<li>
<p><strong>From Entity-Centric to Event-Centric Multimodal Event Knowledge Acquisition</strong> <br>
EE CS Rising Star, University of Texas at Austin, USA. Oct 2022. <br></p>
</li>
<li>
<p><strong>Towards Accurate Intelligent Analysis: Event-Centric Multimedia Knowledge Extraction</strong> <br>
DARPA Forward (Invite-Only), USA. Oct 2022.</p>
</li>
<li>
<p><strong>Event-Centric Multimedia Data Understanding</strong> <br>
Ohio State University, USA. Oct 2022.<br>
Singapore Management University, Singapore. Oct 2022. <br>
George Mason University, USA. Oct 2022. <br>
North Carolina State University, USA. Oct 2022. <br></p>
</li>
<li>
<p><strong>Multimedia Event Extraction: From Object-Centric to Event-Centric</strong> <br>
Virginia Tech, USA. Sept 2022.</p>
</li>
<li>
<p><strong>Event Knowledge Graph Construction</strong> <br>
LOGS Graph Reasoning Seminar. Aug 2022.</p>
</li>
<li>
<p><strong>Event Graph Structures in Vision-Language Understanding</strong> <br>
DataFun. Jun 2022. <br></p>
</li>
<li>
<p><strong>Connecting Vision and Text using Event Structures</strong> <br>
NewsBreak. Apr 2022. <br></p>
</li>
<li>
<p><strong>Memories as Repositories of Events: Structural Event Knowledge Acquisition</strong> <br>
University of Notre Dame. Feb 2022. <br></p>
</li>
<li>
<p><strong>Comprehensive Event Understanding in Multimedia Data</strong> <br>
USC ISI. Dec 2021. <br></p>
</li>
<li>
<p><strong>Structural Event Knowledge Acquisition from Multimedia Data</strong> <br>
UIUC NLP Seminar. Nov 2021. <br></p>
</li>
<li>
<p><strong>Event Extraction and Reasoning in Multimedia News Data</strong> <br>
Microsoft Research. Nov 2021.  <br></p>
</li>
<li>
<p><strong>Improving Visual Event and Argument Role Understanding with Contrastive Image-Language Pretraining</strong>  <br>
Microsoft Research. Aug 2021. <br></p>
</li>
<li>
<p><strong>Fine-Grained Knowledge Extraction System from Multimedia Data</strong> <br>
ai.science. Oct 2020. <br></p>
</li>
<li>
<p><strong>Event Understanding and Narration for Multimedia Data</strong>  <br>
Intel MDI Research Lab. May 2020. <br></p>
</li>
</ul>

    </div>
  </div>


  

  </div>
</section>


  

























































<section id="services" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="row">
    <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Academic Service</h1>
            
    </div>
    <div class="col-12 col-lg-8">
        <p><strong>Organizing Committee</strong>:</p>
<ul>
<li>ACL 2025 (Virtual Infrastructure Chairs)</li>
<li>NAACL 2025 (Publication Chairs)</li>
<li>EMNLP 2024 (Demonstration Chairs)</li>
<li>Foundation Models Meet Embodied Agents @ CVPR 2025 workshop</li>
<li>Towords Knowledgeable Foundation Models @ ACL 2024 workshop</li>
<li>Knowledge Discovery from Unstructured Data in Financial Services (KDF) Workshop</li>
</ul>
<p><strong>Area Chair</strong>:</p>
<ul>
<li>ACL (from 2023)</li>
<li>EMNLP (from 2023)</li>
<li>NAACL (from 2024)</li>
</ul>
<!-- **Senior Program Committee**: 
  * IJCAI, 2021 -->
<p><strong>Program Committee</strong>:</p>
<ul>
<li>ARR (ACL Rolling Review, from 2021)</li>
<li>ACL (from 2021)</li>
<li>EMNLP (from 2021)</li>
<li>NAACL-HLT (from 2021)</li>
<li>AAAI (from 2021)</li>
<li>WWW (from 2021)</li>
<li>AKBC (2021)</li>
<li>EACL (from 2021)</li>
<li>KDD DI Workshop (2021)</li>
<li>NLPCC (2021)</li>
<li>COLING (from 2020)</li>
<li>AACL (from 2020)</li>
<li>CCL (from 2020)</li>
</ul>
<p><strong>Journal Reviewer</strong>:</p>
<ul>
<li>TACL</li>
<li>TIST</li>
<li>TKDE</li>
</ul>
  <!-- * IEEE MultiMedia
  * Journal of Artificial Intelligence Research
  * Journal of Advances in Information Technology -->
<p><strong>Community Services</strong>:</p>
<ul>
<li>ACL Student Research Workshop (SRW) Mentor, ACL, 2024</li>
<li>ACM Mentor, ACM Mentorship Program at UIUC, 2022</li>
<li>CS Ambassador, UIUC CS Visit Day for Prospective Graduate Students, 2022</li>
<li>Advising Assistant, UIUC PhD Orientation Seminar, 2021</li>
<li>Graduate Student Representative, UIUC CS Visit Day, 2020</li>
</ul>
  <!-- * Co-Chair, "Source" Department Technology Association in University of Science and Technology Beijing, 2014 -->
<!-- Volunteer,  the 22nd APEC (Asia-Pacific Economic Cooperation) Economic Leaders' Meeting, 2014 -->
    </div>
  </div>


  

  </div>
</section>


  

























































<section id="awards" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="row">
    <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Awards</h1>
            
    </div>
    <div class="col-12 col-lg-8">
        <h3 id="awards">Awards</h3>
<p><i class="fa-li fas fa-award"></i> ACL 2024 Outstanding Paper Award <br>
<i class="fa-li fas fa-award"></i> SoCal NLP 2024 Best Paper Award <br>
<i class="fa-li fas fa-award"></i> NAACL 2021 Best Demo Paper Award <br>
<i class="fa-li fas fa-award"></i> ACL 2020 Best Demo Paper Award <br>
<i class="fa-li fas fa-award"></i> AAAI 2025 New Faculty Highlights <br>
<i class="fa-li fas fa-award"></i> Microsoft Research Postdoc Fellowship 2023 <br>
<i class="fa-li fas fa-award"></i> EE CS Rising Star 2022 <br>
<i class="fa-li fas fa-award"></i> DARPA Riser 2022<br>
<i class="fa-li fas fa-award"></i> Microsoft Research PhD Fellowship 2021 <br>
<i class="fa-li fas fa-award"></i> Mavis Future Faculty Fellow <br>
<i class="fa-li fas fa-award"></i> C.L. and Jane Liu Award <br>
<i class="fa-li fas fa-award"></i> National Scholarship <br></p>
<!-- <i class="fa-li fas fa-award"></i> Outstanding Undergraduate Award in Beijing <br>
<i class="fa-li fas fa-award"></i> Chinese Academy of Sciences First-Class Scholarship <br> -->
<!-- <i class="fa-li fas fa-award"></i> Schlumberger Scholarship <br> -->
<h3 id="academic-and-scientific-competitions">Academic and Scientific Competitions</h3>
<p><i class="fa-li fas fa-award"></i> Ranked 1st in <a href="https://tac.nist.gov/2020/KBP/SM-KBP/index.html" target="_blank" rel="noopener">NIST TAC Streaming Multimedia Knowledge Base Population (SM-KBP) 2020</a> <br>
<i class="fa-li fas fa-award"></i> Ranked 1st in <a href="https://tac.nist.gov/2019/SM-KBP/index.html" target="_blank" rel="noopener">NIST TAC Streaming Multimedia Knowledge Base Population (SM-KBP) 2019</a>, with more than 10% absolute gains compared the second ranked team <br></p>
<!--Ranked 2nd in TAC Knowledge Base Population (KBP) 2016 -->
<!-- <i class="fa-li fas fa-award"></i> 1st prize in <a href='http://computergames.caai.cn/'>Chinese National Computer Gaming Theory Tournament</a> <br>
<i class="fa-li fas fa-award"></i> 1st prize in <a href='http://dasai.lanqiao.cn/pages/dasai/curren_item.html'>Chinese National Software Innovation Competition</a>  <br> -->
<!-- <i class="fa-li fas fa-award"></i> 2nd prize in <a href='http://jsjds.ruc.edu.cn/'>National Computer Design Competition</a>  <br> -->
<!-- <i class="fa-li fas fa-award"></i> 2nd prize in Student Research Training Program of China <br> -->
<!--3rd prize in National Information Security Competition <br> -->
<!--3rd prize in National Trail of International Contest of innovation (iCAN'13)  <br> -->
<!-- <i class="fa-li fas fa-award"></i> 1st prize in Beijing Micromouse Competition <br>
<i class="fa-li fas fa-award"></i> 2nd prize in Beijing Physics Experiment Competition <br> -->
    </div>
  </div>


  

  </div>
</section>


  

























































<section id="teaching" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="row">
    <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Teaching</h1>
            
    </div>
    <div class="col-12 col-lg-8">
        <!-- **Teaching Assistant**: -->
  <!-- * CS 598 Knowledge-driven Natural Language Generation, Spring 2022  -->
<!-- **Advising Assistant**:  -->
  <!-- * CS 591 PhD Orientation Seminar, Fall 2021  -->
<p><strong>Instructor</strong>:</p>
<ul>
<li><strong>COMP_SCI 396 Reasoning and Planning in the Foundation Model Era</strong><br> NU, Winter 2025</li>
<li><strong>COMP_SCI 496 Agentic AI</strong><br> NU, Spring 2025</li>
</ul>
<p><strong>Guest Lecturer</strong>:</p>
<ul>
<li><strong>Navigating Up and Down in the Job Searching</strong><br> COMP_SCI 496 Academic Job Search <br> NU, Fall 2023</li>
<li><strong>Event-Centric Multimedia Encoding</strong><br> CS 6604 Advanced Topics in Natural Language Processing<br> Virginia Tech, Fall 2022 <!-- - **Event-Centric Multimedia Encoding**<br> CSC 791\&591: Advanced Topics in Efficient Deep Learning<br> North Carolina State University, Fall 2022 --></li>
<li><strong>Knowledge-Driven Vision-Language Pretraining</strong><br> CS 546 Advanced Topics in Natural Language Processing<br> UIUC, Fall 2022</li>
<li><strong>Recent Advances in Multimedia Encoding</strong><br> CS 546 Advanced Topics in Natural Language Processing<br> UIUC, Fall 2022</li>
<li><strong>Timeline Summarization: Introducing Temporal Dimensions into Summarization</strong><br> CS 598 Knowledge Driven Natural Language Generation<br> UIUC, Spring 2022</li>
<li><strong>Multimedia Encoding via Vision-Language Pretraining</strong><br> CS 546 Advanced Topics in Natural Language Processing<br> UIUC, Fall 2021</li>
</ul>

    </div>
  </div>


  

  </div>
</section>


  

























































<section id="students" class="home-section wg-peopleme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    











<div class="row justify-content-center people-widget">
  
  <div class="col-md-12 section-heading">
    <h3>Students</h3>
    <p>It is a great pleasure to work with such talented young people. <br> I am grateful for the trust that they have placed in me.</p>
  </div>
  

  

  
  

  

  
  
  

  

  
  
  

  

  
  
</div>

  

  </div>
</section>


  

























































<section id="alumni" class="home-section wg-markdownme  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    













  <div class="col-12">
    <h3 style="text-align:center;">Past Members</h3>
<p style="text-align:center;"><i class="fa-li fas fa-smile">ACL 2024 Outstanding Paper Award → </i>
  </div>


  

  </div>
</section>




  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  <p class="powered-by copyright-license-text">
    Copyright © Manling Li. Updated 2025.
  </p>
  





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":false}</script>












  
  


<script src="/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
