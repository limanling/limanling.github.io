<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researchsummaries | Manling Li</title>
    <link>https://limanling.github.io/researchsummary/</link>
      <atom:link href="https://limanling.github.io/researchsummary/index.xml" rel="self" type="application/rss+xml" />
    <description>Researchsummaries</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://limanling.github.io/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_512x512_fill_lanczos_center_3.png</url>
      <title>Researchsummaries</title>
      <link>https://limanling.github.io/researchsummary/</link>
    </image>
    
    <item>
      <title>Read: Complex Multimodal Semantics</title>
      <link>https://limanling.github.io/researchsummary/extraction/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://limanling.github.io/researchsummary/extraction/</guid>
      <description>&lt;p&gt;Understanding &lt;strong&gt;Multimodal Semantic Structures&lt;/strong&gt; to answer &lt;em&gt;What happened?&lt;/em&gt;, &lt;em&gt;Who?&lt;/em&gt;, &lt;em&gt;Where?&lt;/em&gt;, and &lt;em&gt;When?&lt;/em&gt; : &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Due to the structural nature and lack of anchoring in a specific image region, abstract semantic structures are difficult to synthesize between text and vision modalities through general large-scale pretraining.&lt;/p&gt;
&lt;p&gt;As the first to &lt;strong&gt;introduce complex event semantic structures into vision-language pretraining&lt;/strong&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP-Event&lt;/a&gt;, I propose a &lt;strong&gt;zero-shot cross-modal transfer&lt;/strong&gt; of semantic understanding abilities from language to vision, which resolves the poor portability issue and supports &lt;strong&gt;Zero-shot Multimodal Event Extraction&lt;/strong&gt; &lt;a href=&#34;https://aclanthology.org/2020.acl-main.230/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M&lt;sup&gt;2&lt;/sup&gt;E&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; for the first time.&lt;/p&gt;
&lt;p&gt;I led a 19-student team collaborating with professors from 4 universities to the development of open-source Multimodal IE system &lt;a href=&#34;https://aclanthology.org/2020.acl-demos.11v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAIA&lt;/a&gt;, which was awarded &lt;strong&gt;ACL 2020 Best Demo Paper&lt;/strong&gt;, and &lt;strong&gt;ranked 1&lt;sup&gt;st&lt;/sup&gt;&lt;/strong&gt; in DARPA AIDA TA1 evaluation each phase since 2019.&lt;/p&gt;
&lt;p&gt;Our COVID knowledge graph &lt;a href=&#34;https://aclanthology.org/2021.naacl-demos.8.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COVID-KG&lt;/a&gt; was awarded &lt;strong&gt;NAACL 2021 Best Demo Paper&lt;/strong&gt;; this work was used to generate a drug re-purposing report during collaborations with Prof David Liem from UCLA Data Science in Cardiovascular Medicine; it has also been widely used by other researchers (our extracted knowledge graph COVID-KG has been downloaded more than 2000 times since 2021).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Think: Long-Horizon Dynamics</title>
      <link>https://limanling.github.io/researchsummary/reasoning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://limanling.github.io/researchsummary/reasoning/</guid>
      <description>&lt;p&gt;Event extraction from massive multimedia data enables us to obtain a large number of historical events.
These historical events imply knowledge about event interactions, which guides our predictions as to
what might happen next and what events are missing. For example, after an ATTACK event there will
usually be ARREST and SENTENCE events. We refer to this knowledge as Event Schemas, which can
be viewed as &amp;ldquo;complex event templates&amp;rdquo; that encode knowledge of stereotypical event structures and
show the progression of event evolution. However, previous event schema induction work has been
oversimplified to be local and sequential, rendering them incapable of dealing with real-world events
with numerous participants, complicated timelines, and various alternative outcomes.&lt;/p&gt;
&lt;p&gt;My research tackles this issue with a new paradigm of event schema knowledge: an Event Graph
Schema, which is a graph-based schema representation that encompasses events, arguments,
temporal connections and argument relations. Figure 1 shows an example schema of the complex event
type car-bombing: a person learns to make bombs, purchasing materials and a vehicle; a bomb is fixed to
the vehicle; and the attacker drives the vehicle to attack people. In this scenario, people can be hurt by
the vehicle or by the bomb’s explosion. It is the first application of graph generation to induce event
schemas and predict future events.&lt;/p&gt;
&lt;p&gt;My work is a new step towards the semantic understanding of inter-event connections. Different from
traditional methods using one-hop relations as connections between events, I learn a complicated graph
including temporal dynamics and multiple paths involving entities (coreferential or related arguments)
that play important roles in a coherent story. Compared to traditional schemas, my new paradigm of Models as Schemas add predictive power to produce multiple hypotheses with probabilities, along
with structural justifications for participant-specific and attribute-specific connections.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Write: Truthfully</title>
      <link>https://limanling.github.io/researchsummary/smartbook/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://limanling.github.io/researchsummary/smartbook/</guid>
      <description>&lt;p&gt;One of the main bottlenecks of large corpora analysis is the multi-document encoding. Whereas existing
studies have built text graphs by augmenting text sequences with different hidden structural information,
they are typically entity-centric and overlook the events’ intra-structures (arguments) and inter-structures
(event-event connections).&lt;/p&gt;
&lt;p&gt;My solution is using event graphs to provide a new comprehensive representation and necessary
inductive bias. I propose to define the multi-document joint representation as the contextualized
embeddings of the nodes on the event graph and collectively model events and arguments. These event
graphs can then be used to address the massive unstructured data challenge in real-world applications:
(1) Timeline Summarization is formulated as an event graph compression problem and then I
design time-aware optimal transport to obtain the summary graph. (2) Meeting Summarization
leverages agenda-based topics to segment meeting transcripts, and takes advantage of multi-modal
sensing of the meeting environment, such as cameras to capture each participant’s head pose and eye
gaze. (3) Multimedia News Question Answering employs multimedia event graphs to condition
synthetic question-answer generation, and to automatically augment data via weak supervision.&lt;/p&gt;
&lt;p&gt;My work is the first study to use event graph representations to overcome fundamental challenges in
handling massive unstructured data that exist in various applications. It provides tangible guidelines to
use event structural knowledge in practice, and shows positive results on long-standing open problems
in event tracking&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://limanling.github.io/researchsummary/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://limanling.github.io/researchsummary/test/</guid>
      <description>&lt;h1 id=&#34;contenthomeportfoliomemd&#34;&gt;content/home/portfoliome.md&lt;/h1&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-custom-portfolio-section-created-with-the-portfolio-widget&#34;&gt;A custom Portfolio section created with the Portfolio widget&lt;/h1&gt;
&lt;p&gt;widget: portfolio&lt;/p&gt;
&lt;h1 id=&#34;this-file-represents-a-page-section&#34;&gt;This file represents a page section.&lt;/h1&gt;
&lt;p&gt;headless: true&lt;/p&gt;
&lt;h1 id=&#34;order-that-this-section-appears-on-the-page&#34;&gt;Order that this section appears on the page.&lt;/h1&gt;
&lt;p&gt;weight: 50&lt;/p&gt;
&lt;h1 id=&#34;section-title&#34;&gt;Section title&lt;/h1&gt;
&lt;p&gt;title: &#39;&#39;&lt;/p&gt;
&lt;h1 id=&#34;section-subtitle&#34;&gt;Section subtitle&lt;/h1&gt;
&lt;p&gt;subtitle: &#39;&#39;&lt;/p&gt;
&lt;p&gt;content:&lt;/p&gt;
&lt;h1 id=&#34;page-type-to-display-eg-project&#34;&gt;Page type to display. E.g. project.&lt;/h1&gt;
&lt;p&gt;page_type: project&lt;/p&gt;
&lt;h1 id=&#34;default-filter-index-eg-0-corresponds-to-the-first-filter_button-instance-below&#34;&gt;Default filter index (e.g. 0 corresponds to the first &lt;code&gt;filter_button&lt;/code&gt; instance below).&lt;/h1&gt;
&lt;p&gt;filter_default: 0&lt;/p&gt;
&lt;h1 id=&#34;filter-toolbar-optional&#34;&gt;Filter toolbar (optional).&lt;/h1&gt;
&lt;h1 id=&#34;add-or-remove-as-many-filters-filter_button-instances-as-you-like&#34;&gt;Add or remove as many filters (&lt;code&gt;filter_button&lt;/code&gt; instances) as you like.&lt;/h1&gt;
&lt;h1 id=&#34;to-show-all-items-set-tag-to-&#34;&gt;To show all items, set &lt;code&gt;tag&lt;/code&gt; to &amp;ldquo;*&amp;rdquo;.&lt;/h1&gt;
&lt;h1 id=&#34;to-filter-by-a-specific-tag-set-tag-to-an-existing-tag-name&#34;&gt;To filter by a specific tag, set &lt;code&gt;tag&lt;/code&gt; to an existing tag name.&lt;/h1&gt;
&lt;h1 id=&#34;to-remove-the-toolbar-delete-the-entire-filter_button-block&#34;&gt;To remove the toolbar, delete the entire &lt;code&gt;filter_button&lt;/code&gt; block.&lt;/h1&gt;
&lt;h1 id=&#34;for-your-specific-requirements&#34;&gt;For your specific requirements:&lt;/h1&gt;
&lt;p&gt;sort_ascending: true&lt;/p&gt;
&lt;h1 id=&#34;filter-only-content-in-specific-folders&#34;&gt;Filter only content in specific folders&lt;/h1&gt;
&lt;p&gt;folders:
- researchsummary&lt;/p&gt;
&lt;p&gt;design:&lt;/p&gt;
&lt;h1 id=&#34;choose-how-many-columns-the-section-has-valid-values-1-or-2&#34;&gt;Choose how many columns the section has. Valid values: &amp;lsquo;1&amp;rsquo; or &amp;lsquo;2&amp;rsquo;.&lt;/h1&gt;
&lt;p&gt;columns: &amp;lsquo;1&amp;rsquo;&lt;/p&gt;
&lt;h1 id=&#34;for-showcase-view-flip-alternate-rows&#34;&gt;For Showcase view, flip alternate rows?&lt;/h1&gt;
&lt;p&gt;flip_alt_rows: true&lt;/p&gt;
&lt;h1 id=&#34;choose-a-view-for-the-listings&#34;&gt;Choose a view for the listings:&lt;/h1&gt;
&lt;p&gt;view: masonry&lt;/p&gt;
&lt;h1 id=&#34;background-colorimage-options&#34;&gt;Background color/image options&lt;/h1&gt;
&lt;p&gt;background: {}&lt;/p&gt;
&lt;h1 id=&#34;spacing-custom-padding&#34;&gt;Spacing (custom padding)&lt;/h1&gt;
&lt;h2 id=&#34;padding-0-0-0-0&#34;&gt;spacing:
padding: [0, 0, 0, 0]&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
