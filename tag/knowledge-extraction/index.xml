<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowledge Extraction | Manling Li</title>
    <link>https://limanling.github.io/tag/knowledge-extraction/</link>
      <atom:link href="https://limanling.github.io/tag/knowledge-extraction/index.xml" rel="self" type="application/rss+xml" />
    <description>Knowledge Extraction</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://limanling.github.io/media/icon_hu437e05b1b543de801281f81b261d0785_1145_512x512_fill_lanczos_center_3.png</url>
      <title>Knowledge Extraction</title>
      <link>https://limanling.github.io/tag/knowledge-extraction/</link>
    </image>
    
    <item>
      <title>Read: Complex Multimodal Semantics</title>
      <link>https://limanling.github.io/researchsummary/extraction/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://limanling.github.io/researchsummary/extraction/</guid>
      <description>&lt;p&gt;Understanding &lt;strong&gt;Multimodal Semantic Structures&lt;/strong&gt; to answer &lt;em&gt;What happened?&lt;/em&gt;, &lt;em&gt;Who?&lt;/em&gt;, &lt;em&gt;Where?&lt;/em&gt;, and &lt;em&gt;When?&lt;/em&gt; : &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Due to the structural nature and lack of anchoring in a specific image region, abstract semantic structures are difficult to synthesize between text and vision modalities through general large-scale pretraining.&lt;/p&gt;
&lt;p&gt;As the first to &lt;strong&gt;introduce complex event semantic structures into vision-language pretraining&lt;/strong&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP-Event&lt;/a&gt;, I propose a &lt;strong&gt;zero-shot cross-modal transfer&lt;/strong&gt; of semantic understanding abilities from language to vision, which resolves the poor portability issue and supports &lt;strong&gt;Zero-shot Multimodal Event Extraction&lt;/strong&gt; &lt;a href=&#34;https://aclanthology.org/2020.acl-main.230/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M&lt;sup&gt;2&lt;/sup&gt;E&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; for the first time.&lt;/p&gt;
&lt;p&gt;I led a 19-student team collaborating with professors from 4 universities to the development of open-source Multimodal IE system &lt;a href=&#34;https://aclanthology.org/2020.acl-demos.11v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAIA&lt;/a&gt;, which was awarded &lt;strong&gt;ACL 2020 Best Demo Paper&lt;/strong&gt;, and &lt;strong&gt;ranked 1&lt;sup&gt;st&lt;/sup&gt;&lt;/strong&gt; in DARPA AIDA TA1 evaluation each phase since 2019.&lt;/p&gt;
&lt;p&gt;Our COVID knowledge graph &lt;a href=&#34;https://aclanthology.org/2021.naacl-demos.8.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COVID-KG&lt;/a&gt; was awarded &lt;strong&gt;NAACL 2021 Best Demo Paper&lt;/strong&gt;; this work was used to generate a drug re-purposing report during collaborations with Prof David Liem from UCLA Data Science in Cardiovascular Medicine; it has also been widely used by other researchers (our extracted knowledge graph COVID-KG has been downloaded more than 2000 times since 2021).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
